{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmallCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SGD classifer is not enough to get reliable insights on CIFAR-10 dataset so we will use a lightweight CNN. This will allow us to accurately estimate the influence of training order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "cifar_train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "cifar_test_dataset  = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try as the Vanilla Base Case: Train on the entire dataset using uniform random shuffling for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "vanilla_model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vanilla_model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(cifar_train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(cifar_test_dataset, batch_size=256)\n",
    "\n",
    "num_epochs = 10\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vanilla_model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vanilla_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    def evaluate(model, loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(vanilla_model, train_loader)\n",
    "    test_acc = evaluate(vanilla_model, test_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As curriculum learning is based on giving samples in increasing difficulty level to the model, we first need to define a difficulty function. We will base ours on difference between the distance of each point to the line that goes through all the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cifar_difficulty(dataset):\n",
    "    \"\"\" Simple proxy: L2 distance to class centroid (in pixel space) \"\"\"\n",
    "    data_loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images = images.view(images.size(0), -1)  # Flatten images\n",
    "\n",
    "    centroids = [images[labels == i].mean(dim=0) for i in range(10)]\n",
    "    difficulty = torch.tensor([\n",
    "        torch.norm(img - centroids[label]).item()\n",
    "        for img, label in zip(images, labels)\n",
    "    ])\n",
    "    return difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Compute difficulty and sort\n",
    "difficulty = compute_cifar_difficulty(cifar_train_dataset)\n",
    "sorted_indices = torch.argsort(difficulty)  # Ascending: easiest to hardest\n",
    "\n",
    "# Difficulty curriculum range\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    n_samples = int(pct * len(cifar_train_dataset))\n",
    "    selected_indices = sorted_indices[:n_samples]\n",
    "    subset = Subset(cifar_train_dataset, selected_indices)\n",
    "    loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Train for 1 epoch on this subset\n",
    "    model.train()\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(subset, batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_dataset, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    print(f\"Trained on {pct:.1%} data -> Train Acc: {train_acc:.2f}, Test Acc: {test_acc:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.plot(percentages, train_accs, label=\"Train Accuracy\")\n",
    "plt.plot(percentages, test_accs, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Training Set Percentage\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Curriculum Learning on CIFAR-10 (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Self-Paced Learning, the model is supposed to:\n",
    "\n",
    "• learn from easier samples first (based on current loss)\n",
    "\n",
    "• adaptively expand its training set to include harder samples as it becomes more confident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac GPU code\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M1/M2 GPU via MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # Per-sample loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "batch_size = 128\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training with {int(pct * 100)}% easiest samples ---\")\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    sample_indices = []\n",
    "\n",
    "    loader = DataLoader(cifar_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            batch_losses = criterion(outputs, labels)\n",
    "            losses.extend(batch_losses.cpu().numpy())\n",
    "            sample_indices.extend(range(idx * batch_size, idx * batch_size + len(images)))\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    sample_indices = np.array(sample_indices)\n",
    "    sorted_idx = sample_indices[np.argsort(losses)]\n",
    "    n_samples = int(pct * len(cifar_train_dataset))\n",
    "    selected_indices = sorted_idx[:n_samples]\n",
    "\n",
    "    model.train()\n",
    "    subset_loader = DataLoader(Subset(cifar_train_dataset, selected_indices), batch_size=batch_size, shuffle=True)\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_dataset, selected_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_dataset, batch_size=256))\n",
    "\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train acc: {train_acc:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Training Set Percentage\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Self-Paced Learning on CIFAR-10 (Small CNN)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard-Example Mining consists in feeding the model only hard examples. In our case, we will consider that a sample is difficult if its normalized difficulty is greater or equal than 0,75 (in other words the top 25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # needed for per-sample loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "percentages = np.linspace(0.1, 1.0, 10)  # percent of hardest samples to train on\n",
    "batch_size = 128\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training with top {int(pct * 100)}% hardest samples ---\")\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    sample_indices = []\n",
    "\n",
    "    loader = DataLoader(cifar_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            batch_losses = criterion(outputs, labels)\n",
    "            losses.extend(batch_losses.cpu().numpy())\n",
    "            sample_indices.extend(range(idx * batch_size, idx * batch_size + len(images)))\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    sample_indices = np.array(sample_indices)\n",
    "    sorted_idx = sample_indices[np.argsort(-losses)]  # descending\n",
    "    n_samples = int(pct * len(cifar_train_dataset))\n",
    "    selected_indices = sorted_idx[:n_samples]\n",
    "\n",
    "    model.train()\n",
    "    subset_loader = DataLoader(Subset(cifar_train_dataset, selected_indices), batch_size=batch_size, shuffle=True)\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_dataset, selected_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_dataset, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Top % Hardest Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Hard Example Mining on CIFAR-10 (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are implementing **Reverse Curriculum Learning (RCL)** where the model starts learning from easier goals that are close to the target and gradually works backwards to more challenging starting states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # per-sample loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 128\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "difficulty = compute_cifar_difficulty(cifar_train_dataset)\n",
    "sorted_indices = torch.argsort(difficulty, descending=True)  # hardest first\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training with top {int(pct * 100)}% hardest samples ---\")\n",
    "\n",
    "    n_samples = int(pct * len(cifar_train_dataset))\n",
    "    selected_indices = sorted_indices[:n_samples]\n",
    "    subset_loader = DataLoader(Subset(cifar_train_dataset, selected_indices), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_dataset, selected_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_dataset, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# plot\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Top % Hardest Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Reverse Curriculum Learning on CIFAR-10 (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified Monte Carlo Sampling** is a variance reduction technique where the input space is divided into distinct strata (subregions), and samples are drawn from each stratum. This ensures more uniform coverage of the space compared to standard Monte Carlo sampling, leading to more accurate and stable estimates with fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "difficulty = compute_cifar_difficulty(cifar_train_dataset)\n",
    "difficulty = (difficulty - difficulty.min()) / (difficulty.max() - difficulty.min())  # Normalize to [0, 1]\n",
    "\n",
    "# Stratify into bins\n",
    "num_bins = 10\n",
    "bin_edges = torch.linspace(0, 1, num_bins + 1)\n",
    "bin_indices = [[] for _ in range(num_bins)]\n",
    "\n",
    "for i, d in enumerate(difficulty):\n",
    "    bin_id = torch.bucketize(d, bin_edges, right=False) - 1\n",
    "    bin_indices[bin_id.item()].append(i)\n",
    "\n",
    "# Training loop\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 128\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training on {int(pct * 100)}% stratified samples ---\")\n",
    "    samples_per_bin = int(pct * len(cifar_train_dataset) / num_bins)\n",
    "\n",
    "    sampled_indices = []\n",
    "    for indices in bin_indices:\n",
    "        sampled = np.random.choice(indices, min(samples_per_bin, len(indices)), replace=False)\n",
    "        sampled_indices.extend(sampled)\n",
    "\n",
    "    subset_loader = DataLoader(Subset(cifar_train_dataset, sampled_indices), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_dataset, sampled_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_dataset, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Training Set Percentage (Stratified)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Stratified Monte Carlo Sampling on CIFAR-10 (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset with Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train_GN = None\n",
    "cifar_test_GN = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "vanilla_model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vanilla_model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(cifar_train_GN, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(cifar_test_GN, batch_size=256)\n",
    "\n",
    "num_epochs = 10\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vanilla_model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vanilla_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    def evaluate(model, loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(vanilla_model, train_loader)\n",
    "    test_acc = evaluate(vanilla_model, test_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cifar_difficulty(dataset):\n",
    "    \"\"\" Simple proxy: L2 distance to class centroid (in pixel space) \"\"\"\n",
    "    data_loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images = images.view(images.size(0), -1)  # Flatten images\n",
    "\n",
    "    centroids = [images[labels == i].mean(dim=0) for i in range(10)]\n",
    "    difficulty = torch.tensor([\n",
    "        torch.norm(img - centroids[label]).item()\n",
    "        for img, label in zip(images, labels)\n",
    "    ])\n",
    "    return difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Compute difficulty and sort\n",
    "difficulty = compute_cifar_difficulty(cifar_train_GN)\n",
    "sorted_indices = torch.argsort(difficulty)  # Ascending: easiest to hardest\n",
    "\n",
    "# Difficulty curriculum range\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    n_samples = int(pct * len(cifar_train_GN))\n",
    "    selected_indices = sorted_indices[:n_samples]\n",
    "    subset = Subset(cifar_train_GN, selected_indices)\n",
    "    loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Train for 1 epoch on this subset\n",
    "    model.train()\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(subset, batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_dataset, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    print(f\"Trained on {pct:.1%} data -> Train Acc: {train_acc:.2f}, Test Acc: {test_acc:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.plot(percentages, train_accs, label=\"Train Accuracy\")\n",
    "plt.plot(percentages, test_accs, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Training Set Percentage\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Curriculum Learning on CIFAR-10 with Gaussian Noise (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac GPU code\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M1/M2 GPU via MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # Per-sample loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "batch_size = 128\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training with {int(pct * 100)}% easiest samples ---\")\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    sample_indices = []\n",
    "\n",
    "    loader = DataLoader(cifar_train_GN, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            batch_losses = criterion(outputs, labels)\n",
    "            losses.extend(batch_losses.cpu().numpy())\n",
    "            sample_indices.extend(range(idx * batch_size, idx * batch_size + len(images)))\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    sample_indices = np.array(sample_indices)\n",
    "    sorted_idx = sample_indices[np.argsort(losses)]\n",
    "    n_samples = int(pct * len(cifar_train_GN))\n",
    "    selected_indices = sorted_idx[:n_samples]\n",
    "\n",
    "    model.train()\n",
    "    subset_loader = DataLoader(Subset(cifar_train_GN, selected_indices), batch_size=batch_size, shuffle=True)\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_GN, selected_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_GN, batch_size=256))\n",
    "\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train acc: {train_acc:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Training Set Percentage\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Self-Paced Learning on CIFAR-10 with Gaussian Noise (Small CNN)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # needed for per-sample loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "percentages = np.linspace(0.1, 1.0, 10)  # percent of hardest samples to train on\n",
    "batch_size = 128\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training with top {int(pct * 100)}% hardest samples ---\")\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    sample_indices = []\n",
    "\n",
    "    loader = DataLoader(cifar_train_GN, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            batch_losses = criterion(outputs, labels)\n",
    "            losses.extend(batch_losses.cpu().numpy())\n",
    "            sample_indices.extend(range(idx * batch_size, idx * batch_size + len(images)))\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    sample_indices = np.array(sample_indices)\n",
    "    sorted_idx = sample_indices[np.argsort(-losses)]  # descending\n",
    "    n_samples = int(pct * len(cifar_train_GN))\n",
    "    selected_indices = sorted_idx[:n_samples]\n",
    "\n",
    "    model.train()\n",
    "    subset_loader = DataLoader(Subset(cifar_train_GN, selected_indices), batch_size=batch_size, shuffle=True)\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_GN, selected_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_GN, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Top % Hardest Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Hard Example Mining on CIFAR-10 with Gaussian Noise (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # per-sample loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 128\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "difficulty = compute_cifar_difficulty(cifar_train_GN)\n",
    "sorted_indices = torch.argsort(difficulty, descending=True)  # hardest first\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training with top {int(pct * 100)}% hardest samples ---\")\n",
    "\n",
    "    n_samples = int(pct * len(cifar_train_GN))\n",
    "    selected_indices = sorted_indices[:n_samples]\n",
    "    subset_loader = DataLoader(Subset(cifar_train_GN, selected_indices), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_GN, selected_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_GN, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# plot\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Top % Hardest Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Reverse Curriculum Learning on CIFAR-10 with Gaussian Noise (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "difficulty = compute_cifar_difficulty(cifar_train_GN)\n",
    "difficulty = (difficulty - difficulty.min()) / (difficulty.max() - difficulty.min())  # Normalize to [0, 1]\n",
    "\n",
    "# Stratify into bins\n",
    "num_bins = 10\n",
    "bin_edges = torch.linspace(0, 1, num_bins + 1)\n",
    "bin_indices = [[] for _ in range(num_bins)]\n",
    "\n",
    "for i, d in enumerate(difficulty):\n",
    "    bin_id = torch.bucketize(d, bin_edges, right=False) - 1\n",
    "    bin_indices[bin_id.item()].append(i)\n",
    "\n",
    "# Training loop\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 128\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training on {int(pct * 100)}% stratified samples ---\")\n",
    "    samples_per_bin = int(pct * len(cifar_train_GN) / num_bins)\n",
    "\n",
    "    sampled_indices = []\n",
    "    for indices in bin_indices:\n",
    "        sampled = np.random.choice(indices, min(samples_per_bin, len(indices)), replace=False)\n",
    "        sampled_indices.extend(sampled)\n",
    "\n",
    "    subset_loader = DataLoader(Subset(cifar_train_GN, sampled_indices), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_GN, sampled_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_GN, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# plot\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Training Set Percentage (Stratified)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Stratified Monte Carlo Sampling on CIFAR-10 with Gaussian Noise (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset with Impulse Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train_IN = None\n",
    "cifar_test_IN = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "vanilla_model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vanilla_model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(cifar_train_IN, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(cifar_test_IN, batch_size=256)\n",
    "\n",
    "num_epochs = 10\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vanilla_model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vanilla_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    def evaluate(model, loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(vanilla_model, train_loader)\n",
    "    test_acc = evaluate(vanilla_model, test_loader)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cifar_difficulty(dataset):\n",
    "    \"\"\" Simple proxy: L2 distance to class centroid (in pixel space) \"\"\"\n",
    "    data_loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images = images.view(images.size(0), -1)  # Flatten images\n",
    "\n",
    "    centroids = [images[labels == i].mean(dim=0) for i in range(10)]\n",
    "    difficulty = torch.tensor([\n",
    "        torch.norm(img - centroids[label]).item()\n",
    "        for img, label in zip(images, labels)\n",
    "    ])\n",
    "    return difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Compute difficulty and sort\n",
    "difficulty = compute_cifar_difficulty(cifar_train_IN)\n",
    "sorted_indices = torch.argsort(difficulty)  # Ascending: easiest to hardest\n",
    "\n",
    "# Difficulty curriculum range\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    n_samples = int(pct * len(cifar_train_IN))\n",
    "    selected_indices = sorted_indices[:n_samples]\n",
    "    subset = Subset(cifar_train_IN, selected_indices)\n",
    "    loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Train for 1 epoch on this subset\n",
    "    model.train()\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(subset, batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_dataset, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    print(f\"Trained on {pct:.1%} data -> Train Acc: {train_acc:.2f}, Test Acc: {test_acc:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.plot(percentages, train_accs, label=\"Train Accuracy\")\n",
    "plt.plot(percentages, test_accs, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Training Set Percentage\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Curriculum Learning on CIFAR-10 with Impulse Noise (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac GPU code\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M1/M2 GPU via MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # Per-sample loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "batch_size = 128\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training with {int(pct * 100)}% easiest samples ---\")\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    sample_indices = []\n",
    "\n",
    "    loader = DataLoader(cifar_train_IN, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            batch_losses = criterion(outputs, labels)\n",
    "            losses.extend(batch_losses.cpu().numpy())\n",
    "            sample_indices.extend(range(idx * batch_size, idx * batch_size + len(images)))\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    sample_indices = np.array(sample_indices)\n",
    "    sorted_idx = sample_indices[np.argsort(losses)]\n",
    "    n_samples = int(pct * len(cifar_train_GN))\n",
    "    selected_indices = sorted_idx[:n_samples]\n",
    "\n",
    "    model.train()\n",
    "    subset_loader = DataLoader(Subset(cifar_train_IN, selected_indices), batch_size=batch_size, shuffle=True)\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_IN, selected_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_IN, batch_size=256))\n",
    "\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train acc: {train_acc:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Training Set Percentage\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Self-Paced Learning on CIFAR-10 with Impulse Noise (Small CNN)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # needed for per-sample loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "percentages = np.linspace(0.1, 1.0, 10)  # percent of hardest samples to train on\n",
    "batch_size = 128\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training with top {int(pct * 100)}% hardest samples ---\")\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    sample_indices = []\n",
    "\n",
    "    loader = DataLoader(cifar_train_IN, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            batch_losses = criterion(outputs, labels)\n",
    "            losses.extend(batch_losses.cpu().numpy())\n",
    "            sample_indices.extend(range(idx * batch_size, idx * batch_size + len(images)))\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    sample_indices = np.array(sample_indices)\n",
    "    sorted_idx = sample_indices[np.argsort(-losses)]  # descending\n",
    "    n_samples = int(pct * len(cifar_train_IN))\n",
    "    selected_indices = sorted_idx[:n_samples]\n",
    "\n",
    "    model.train()\n",
    "    subset_loader = DataLoader(Subset(cifar_train_IN, selected_indices), batch_size=batch_size, shuffle=True)\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_IN, selected_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_IN, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Top % Hardest Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Hard Example Mining on CIFAR-10 with Impulse Noise (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # per-sample loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 128\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "difficulty = compute_cifar_difficulty(cifar_train_IN)\n",
    "sorted_indices = torch.argsort(difficulty, descending=True)  # hardest first\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training with top {int(pct * 100)}% hardest samples ---\")\n",
    "\n",
    "    n_samples = int(pct * len(cifar_train_IN))\n",
    "    selected_indices = sorted_indices[:n_samples]\n",
    "    subset_loader = DataLoader(Subset(cifar_train_GN, selected_indices), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_IN, selected_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_IN, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# plot\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Top % Hardest Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Reverse Curriculum Learning on CIFAR-10 with Impulse Noise (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "difficulty = compute_cifar_difficulty(cifar_train_IN)\n",
    "difficulty = (difficulty - difficulty.min()) / (difficulty.max() - difficulty.min())  # Normalize to [0, 1]\n",
    "\n",
    "# Stratify into bins\n",
    "num_bins = 10\n",
    "bin_edges = torch.linspace(0, 1, num_bins + 1)\n",
    "bin_indices = [[] for _ in range(num_bins)]\n",
    "\n",
    "for i, d in enumerate(difficulty):\n",
    "    bin_id = torch.bucketize(d, bin_edges, right=False) - 1\n",
    "    bin_indices[bin_id.item()].append(i)\n",
    "\n",
    "# Training loop\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 128\n",
    "percentages = np.linspace(0.1, 1.0, 10)\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n--- Training on {int(pct * 100)}% stratified samples ---\")\n",
    "    samples_per_bin = int(pct * len(cifar_train_IN) / num_bins)\n",
    "\n",
    "    sampled_indices = []\n",
    "    for indices in bin_indices:\n",
    "        sampled = np.random.choice(indices, min(samples_per_bin, len(indices)), replace=False)\n",
    "        sampled_indices.extend(sampled)\n",
    "\n",
    "    subset_loader = DataLoader(Subset(cifar_train_IN, sampled_indices), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                preds = model(images).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "    train_acc = evaluate(DataLoader(Subset(cifar_train_IN, sampled_indices), batch_size=256))\n",
    "    test_acc = evaluate(DataLoader(cifar_test_IN, batch_size=256))\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# plot\n",
    "plt.plot(percentages, train_accs, label='Train Accuracy')\n",
    "plt.plot(percentages, test_accs, label='Test Accuracy')\n",
    "plt.xlabel(\"Training Set Percentage (Stratified)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Stratified Monte Carlo Sampling on CIFAR-10 with Impulse Noise (Small CNN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
