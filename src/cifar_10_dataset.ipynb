{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmallCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SGD classifer is not enough to get reliable insights on CIFAR-10 dataset so we will use a lightweight CNN. This will allow us to accurately estimate the influence of training order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset (Vanilla Case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Cifar-10 consist of RGB images, we can define 3 channels of size 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "X_train: torch.Size([50000, 3, 32, 32]), y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32]), y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),   # CIFAR-10 mean\n",
    "                         (0.2023, 0.1994, 0.2010))   # CIFAR-10 std\n",
    "])\n",
    "\n",
    "cifar_train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "cifar_test_dataset  = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "X_train = torch.stack([cifar_train_dataset[i][0] for i in range(len(cifar_train_dataset))])\n",
    "y_train = torch.tensor([cifar_train_dataset[i][1] for i in range(len(cifar_train_dataset))])\n",
    "\n",
    "X_test = torch.stack([cifar_test_dataset[i][0] for i in range(len(cifar_test_dataset))])\n",
    "y_test = torch.tensor([cifar_test_dataset[i][1] for i in range(len(cifar_test_dataset))])\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try as the Vanilla Base Case: Train on the entire dataset using uniform random shuffling for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.2805\n",
      "Epoch 2/10 - Train Loss: 0.8993\n",
      "Epoch 3/10 - Train Loss: 0.7412\n",
      "Epoch 4/10 - Train Loss: 0.6213\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As curriculum learning is based on giving samples in increasing difficulty level to the model, we first need to define a difficulty function. We will base ours on difference between the distance of each point to the line that goes through all the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cifar_difficulty(dataset):    \n",
    "    data_loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images = images.view(images.size(0), -1)  # Flatten images\n",
    "\n",
    "    centroids = [images[labels == i].mean(dim=0) for i in range(10)]\n",
    "    difficulty = torch.tensor([\n",
    "        torch.norm(img - centroids[label]).item()\n",
    "        for img, label in zip(images, labels)\n",
    "    ])\n",
    "    return difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "difficulty = compute_cifar_difficulty(cifar_train_dataset)\n",
    "difficulty = (difficulty - difficulty.min()) / (difficulty.max() - difficulty.min())\n",
    "sorted_indices = np.argsort(difficulty)  # easiest to hardest\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "\n",
    "phases = np.linspace(0.1, 1.0, num_stages)\n",
    "previous_n = 0\n",
    "\n",
    "\n",
    "for stage, phase in enumerate(phases):\n",
    "    current_n = int(phase * len(sorted_indices))\n",
    "    selected_idx = sorted_indices[previous_n:current_n]\n",
    "    previous_n = current_n\n",
    "    \n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using {len(selected_idx)} new examples\")\n",
    "\n",
    "    X_stage = X_train[selected_idx]\n",
    "    y_stage = y_train[selected_idx]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Self-Paced Learning, the model is supposed to:\n",
    "\n",
    "• learn from easier samples first (based on current loss)\n",
    "\n",
    "• adaptively expand its training set to include harder samples as it becomes more confident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 15\n",
    "samples_per_epoch = len(X_train)\n",
    "\n",
    "seen_mask = torch.zeros(len(X_train), dtype=torch.bool)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        unseen_indices = (~seen_mask).nonzero(as_tuple=True)[0]\n",
    "        unseen_loader = DataLoader(TensorDataset(X_train[unseen_indices], y_train[unseen_indices]),\n",
    "                                   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        all_losses = []\n",
    "        for images, labels in unseen_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            losses = criterion(outputs, labels)\n",
    "            all_losses.append(losses.cpu())\n",
    "\n",
    "        all_losses = torch.cat(all_losses)\n",
    "    \n",
    "    k = min(samples_per_epoch, len(unseen_indices))\n",
    "    selected_in_unseen = torch.topk(-all_losses, k).indices\n",
    "    selected_indices = unseen_indices[selected_in_unseen]\n",
    "\n",
    "    seen_mask[selected_indices] = True\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: selected {len(selected_indices)} new samples (total seen: {seen_mask.sum().item()}/{len(X_train)})\")\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train[selected_indices], y_train[selected_indices]),\n",
    "                              batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(labels)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_loader.dataset)\n",
    "    print(f\"  Train Loss: {total_loss / len(train_loader.dataset):.4f}, Acc: {acc:.4f}\")\n",
    "    \n",
    "print(\"Training complete.\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard-Example Mining consists in feeding the model only hard examples. In our case, we will consider that a sample is difficult if its normalized difficulty is greater or equal than 0,60 (in other words the top 40%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2990 hard examples out of 50000 total\n",
      "Epoch 1/10 - Loss: 1.9736, Acc: 0.2953\n",
      "Epoch 2/10 - Loss: 1.5191, Acc: 0.4759\n",
      "Epoch 3/10 - Loss: 1.2759, Acc: 0.5686\n",
      "Epoch 4/10 - Loss: 1.0993, Acc: 0.6120\n",
      "Epoch 5/10 - Loss: 0.9213, Acc: 0.6890\n",
      "Epoch 6/10 - Loss: 0.7678, Acc: 0.7411\n",
      "Epoch 7/10 - Loss: 0.5961, Acc: 0.7990\n",
      "Epoch 8/10 - Loss: 0.5057, Acc: 0.8341\n",
      "Epoch 9/10 - Loss: 0.3545, Acc: 0.8957\n",
      "Epoch 10/10 - Loss: 0.2610, Acc: 0.9268\n",
      "\n",
      "Final Test Accuracy: 0.3156\n"
     ]
    }
   ],
   "source": [
    "difficulty = compute_cifar_difficulty(cifar_train_dataset)\n",
    "difficulty = (difficulty - difficulty.min()) / (difficulty.max() - difficulty.min())\n",
    "\n",
    "hard_mask = difficulty >= 0.60\n",
    "hard_indices = np.where(hard_mask)[0] # we do not shuffle the indices to train on increasingly difficult samples (adapted CL idea)\n",
    "print(f\"Selected {len(hard_indices)} hard examples out of {len(difficulty)} total\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "X_hard = X_train[hard_indices]\n",
    "y_hard = y_train[hard_indices]\n",
    "\n",
    "train_dataset = TensorDataset(X_hard, y_hard)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL TEST ACCURACY : HYPER BAS PARCE QUE 40% DES DONNÉES DIFFICILES = 3/50 % des données jsp si on réadapte du cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are implementing **Reverse Curriculum Learning (RCL)** where the model starts learning from easier goals that are close to the target and gradually works backwards to more challenging starting states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using 5000 new hard→easy examples\n",
      "  Epoch 1/1 - Loss: 1.9906, Acc: 0.2810\n",
      "\n",
      "Stage 2/5: Using 11250 new hard→easy examples\n",
      "  Epoch 1/1 - Loss: 1.4940, Acc: 0.4584\n",
      "\n",
      "Stage 3/5: Using 11250 new hard→easy examples\n"
     ]
    }
   ],
   "source": [
    "difficulty = compute_cifar_difficulty(cifar_train_dataset)\n",
    "difficulty = (difficulty - difficulty.min()) / (difficulty.max() - difficulty.min())\n",
    "difficulty = difficulty.cpu().numpy()\n",
    "\n",
    "sorted_indices = np.argsort(difficulty)[::-1].copy()  # reverse order for hardest first\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1\n",
    "num_stages = 5\n",
    "\n",
    "phases = np.linspace(0.1, 1.0, num_stages)\n",
    "\n",
    "previous_n = 0\n",
    "\n",
    "for stage, phase in enumerate(phases):\n",
    "    current_n = int(phase * len(sorted_indices))\n",
    "    selected_idx = sorted_indices[previous_n:current_n]\n",
    "    previous_n = current_n\n",
    "    \n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using {len(selected_idx)} new hard→easy examples\")\n",
    "\n",
    "    X_stage = X_train[selected_idx]\n",
    "    y_stage = y_train[selected_idx]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified Monte Carlo Sampling** is a variance reduction technique where the input space is divided into distinct strata (subregions), and samples are drawn from each stratum. This ensures more uniform coverage of the space compared to standard Monte Carlo sampling, leading to more accurate and stable estimates with fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty = compute_cifar_difficulty(cifar_train_dataset)\n",
    "difficulty = (difficulty - difficulty.min()) / (difficulty.max() - difficulty.min())\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1\n",
    "num_stages = 5\n",
    "samples_per_stage = int(len(X_train) / num_stages)\n",
    "\n",
    "num_bins = num_stages\n",
    "bin_edges = np.linspace(0, 1, num_bins + 1)\n",
    "bins = [[] for _ in range(num_bins)]\n",
    "\n",
    "for idx, score in enumerate(difficulties):\n",
    "    for b in range(num_bins):\n",
    "        if bin_edges[b] <= score < bin_edges[b + 1] or (b == num_bins - 1 and score == 1.0):\n",
    "            bins[b].append(idx)\n",
    "            break\n",
    "\n",
    "for b in bins:\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "seen_indices = set()\n",
    "num_seen = 0\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    \n",
    "\n",
    "    stage_indices = []\n",
    "\n",
    "    for b in bins:\n",
    "        take_n = min(samples_per_stage // num_bins, len(b))\n",
    "        selected = [i for i in b if i not in seen_indices][:take_n]\n",
    "        seen_indices.update(selected)\n",
    "        stage_indices.extend(selected)\n",
    "\n",
    "    np.random.shuffle(stage_indices)\n",
    "\n",
    "    X_stage = X_train[stage_indices]\n",
    "    y_stage = y_train[stage_indices]\n",
    "    num_seen += len(X_stage)\n",
    "\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Sampling {len(X_stage)} data points from all difficulty strata ({num_seen}/{len(X_train)})\")\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset with Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a range of increasing difficulty. \n",
    "- 0.0: no noise — easiest samples\n",
    "- 0.2: very noisy — hardest samples\n",
    "- 0.25+ usually makes Cifar unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "base_test  = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "full_data = torch.cat([torch.tensor(base_train.data), torch.tensor(base_test.data)], dim=0).float() / 255.0\n",
    "full_targets = torch.cat([\n",
    "    torch.tensor(base_train.targets), \n",
    "    torch.tensor(base_test.targets)\n",
    "], dim=0)\n",
    "\n",
    "full_data = full_data.permute(0, 3, 1, 2)\n",
    "\n",
    "noise_levels = [0.0, 0.05, 0.1, 0.15, 0.2]\n",
    "sigma = 0.02\n",
    "\n",
    "augmented_data, augmented_targets, noise_labels = [], [], []\n",
    "\n",
    "for level in noise_levels:\n",
    "    noisy_data = full_data.clone()\n",
    "    if level > 0:\n",
    "        N, C, H, W = noisy_data.shape\n",
    "        total_pixels = H * W\n",
    "\n",
    "        num_noisy = int(level * total_pixels)\n",
    "\n",
    "        for i in range(N):\n",
    "            coords = torch.randperm(total_pixels)[:num_noisy]\n",
    "            flat_image = noisy_data[i].view(C, -1)\n",
    "            flat_image[:, coords] += torch.randn(C, num_noisy) * sigma\n",
    "            flat_image[:, coords].clamp_(0.0, 1.0)\n",
    "\n",
    "    augmented_data.append(noisy_data)\n",
    "    augmented_targets.append(full_targets)\n",
    "    noise_labels.append(torch.full_like(full_targets, fill_value=level))\n",
    "\n",
    "augmented_data = torch.cat(augmented_data, dim=0)\n",
    "augmented_targets = torch.cat(augmented_targets, dim=0)\n",
    "noise_labels = torch.cat(noise_labels, dim=0)\n",
    "\n",
    "N_train = len(base_train)\n",
    "N_test = len(base_test)\n",
    "samples_per_level = N_train + N_test\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for i in range(len(noise_levels)):\n",
    "    start = i * samples_per_level\n",
    "    end = start + samples_per_level\n",
    "\n",
    "    X_level = augmented_data[start:end]\n",
    "    y_level = augmented_targets[start:end]\n",
    "\n",
    "    X_train_list.append(X_level[:N_train])\n",
    "    y_train_list.append(y_level[:N_train])\n",
    "    X_test_list.append(X_level[N_train:])\n",
    "    y_test_list.append(y_level[N_train:])\n",
    "\n",
    "X_train = torch.cat(X_train_list, dim=0)\n",
    "y_train = torch.cat(y_train_list, dim=0)\n",
    "X_test = torch.cat(X_test_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Create noise level labels for training set\n",
    "noise_levels_train_list = []\n",
    "\n",
    "for i, level in enumerate(noise_levels):\n",
    "    noise_level_tensor = torch.full((N_train,), fill_value=i)\n",
    "    noise_levels_train_list.append(noise_level_tensor)\n",
    "\n",
    "noise_levels_train = torch.cat(noise_levels_train_list, dim=0)\n",
    "print(f\"noise_levels_train shape: {noise_levels_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(base_train) \n",
    "N_test = len(base_test)\n",
    "\n",
    "samples_per_level = N_train + N_test\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for i in range(len(noise_levels)):\n",
    "    start = i * samples_per_level\n",
    "    end = start + samples_per_level\n",
    "\n",
    "    X_level = augmented_data[start:end]\n",
    "    y_level = augmented_targets[start:end]\n",
    "\n",
    "    X_train_list.append(X_level[:N_train])\n",
    "    y_train_list.append(y_level[:N_train])\n",
    "    X_test_list.append(X_level[N_train:])\n",
    "    y_test_list.append(y_level[N_train:])\n",
    "\n",
    "X_train = torch.cat(X_train_list, dim=0)\n",
    "y_train = torch.cat(y_train_list, dim=0)\n",
    "X_test = torch.cat(X_test_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels_train_list = []\n",
    "\n",
    "for i, level in enumerate(noise_levels):\n",
    "    num_train_samples = N_train\n",
    "    noise_level_tensor = torch.full((num_train_samples,), fill_value=i)\n",
    "    \n",
    "    noise_levels_train_list.append(noise_level_tensor)\n",
    "\n",
    "noise_levels_train = torch.cat(noise_levels_train_list, dim=0)\n",
    "\n",
    "print(f\"noise_levels_train shape: {noise_levels_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # random sampling baseline\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_acc = total_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {total_loss/len(train_dataset):.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "test_acc = correct / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.1) Cumulative Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels <= {stage}\")\n",
    "    \n",
    "    stage_mask = noise_levels_train <= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.2) Strict Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAUDRA SANS DOUTE MODIF INITIAL_LAMBDA & LAMBDA_INCREMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')  # Important: per-sample loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "initial_lambda = 1  # initial difficulty threshold\n",
    "lambda_increment = 2  # increase per epoch\n",
    "\n",
    "full_train_dataset = TensorDataset(X_train, y_train)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            losses = criterion(outputs, labels)\n",
    "            all_losses.append(losses.cpu())\n",
    "\n",
    "    all_losses = torch.cat(all_losses)\n",
    "    print(torch.min(all_losses))\n",
    "    print(torch.max(all_losses))\n",
    "\n",
    "    lambda_threshold = initial_lambda + epoch * lambda_increment\n",
    "\n",
    "    selected_indices = (all_losses <= lambda_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"Epoch {epoch+1}: No samples selected for training (lambda={lambda_threshold:.3f}), stopping early.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: lambda={lambda_threshold:.3f}, selected {len(selected_indices)}/{len(X_train)} samples\")\n",
    "\n",
    "    train_subset = TensorDataset(X_train[selected_indices], y_train[selected_indices])\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_subset)\n",
    "    print(f\"  Training Loss: {total_loss/len(train_subset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if len(selected_indices) == len(X_train):\n",
    "        print(f\"  All samples were selected, stopping early.\")\n",
    "        break\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "hard_mask = noise_levels_train >= 3\n",
    "\n",
    "X_hard = X_train[hard_mask]\n",
    "y_hard = y_train[hard_mask]\n",
    "print(f\"Selected {len(X_hard)} hard examples out of {len(X_train)}\")\n",
    "\n",
    "hard_dataset = TensorDataset(X_hard, y_hard)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in hard_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(hard_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(hard_dataset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.1) Cumulative Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels >= {stage}\")\n",
    "    \n",
    "    stage_mask = noise_levels_train >= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.2) Strict Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified Monte Carlo Sampling** is a variance reduction technique where the input space is divided into distinct strata (subregions), and samples are drawn from each stratum. This ensures more uniform coverage of the space compared to standard Monte Carlo sampling, leading to more accurate and stable estimates with fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "samples_per_stage = 10000\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Sampling from stage level = {stage}\")\n",
    "    \n",
    "    stage_mask = (noise_levels_train == stage).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Randomly sample without replacement\n",
    "    if len(stage_mask) < samples_per_stage:\n",
    "        print(f\"  Warning: only {len(stage_mask)} samples available, using all.\")\n",
    "        selected_indices = stage_mask\n",
    "    else:\n",
    "        selected_indices = stage_mask[torch.randperm(len(stage_mask))[:samples_per_stage]]\n",
    "    \n",
    "    X_stage = X_train[selected_indices]\n",
    "    y_stage = y_train[selected_indices]\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset with Impulse Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a range of increasing difficulty. \n",
    "- 0.0: no noise — easiest samples\n",
    "- 0.2: very noisy — hardest samples\n",
    "- 0.25+ usually makes Cifar unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "X_train: torch.Size([250000, 3, 32, 32]), y_train: torch.Size([250000])\n",
      "X_test: torch.Size([50000, 3, 32, 32]), y_test: torch.Size([50000])\n",
      "noise_levels_train shape: torch.Size([250000])\n"
     ]
    }
   ],
   "source": [
    "base_train = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "base_test  = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "full_data = torch.cat([torch.tensor(base_train.data), torch.tensor(base_test.data)], dim=0).float() / 255.0\n",
    "full_targets = torch.cat([\n",
    "    torch.tensor(base_train.targets), \n",
    "    torch.tensor(base_test.targets)\n",
    "], dim=0)\n",
    "\n",
    "full_data = full_data.permute(0, 3, 1, 2)\n",
    "\n",
    "noise_levels = [0.0, 0.05, 0.1, 0.15, 0.2]\n",
    "sigma = 0.02\n",
    "\n",
    "augmented_data, augmented_targets, noise_labels = [], [], []\n",
    "\n",
    "for level in noise_levels:\n",
    "    noisy_data = full_data.clone()\n",
    "    if level > 0:\n",
    "        N, C, H, W = noisy_data.shape\n",
    "        total_pixels = H * W\n",
    "\n",
    "        num_noisy = int(level * total_pixels)\n",
    "\n",
    "        for i in range(N):\n",
    "            coords = torch.randperm(total_pixels)[:num_noisy]\n",
    "            flat_image = noisy_data[i].view(C, -1)\n",
    "            flat_image[:, coords] = torch.randint(0, 2, (C, num_noisy), dtype=torch.float32)    \n",
    "            flat_image[:, coords].clamp_(0.0, 1.0)\n",
    "\n",
    "    augmented_data.append(noisy_data)\n",
    "    augmented_targets.append(full_targets)\n",
    "    noise_labels.append(torch.full_like(full_targets, fill_value=level))\n",
    "\n",
    "augmented_data = torch.cat(augmented_data, dim=0)\n",
    "augmented_targets = torch.cat(augmented_targets, dim=0)\n",
    "noise_labels = torch.cat(noise_labels, dim=0)\n",
    "\n",
    "N_train = len(base_train)\n",
    "N_test = len(base_test)\n",
    "samples_per_level = N_train + N_test\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for i in range(len(noise_levels)):\n",
    "    start = i * samples_per_level\n",
    "    end = start + samples_per_level\n",
    "\n",
    "    X_level = augmented_data[start:end]\n",
    "    y_level = augmented_targets[start:end]\n",
    "\n",
    "    X_train_list.append(X_level[:N_train])\n",
    "    y_train_list.append(y_level[:N_train])\n",
    "    X_test_list.append(X_level[N_train:])\n",
    "    y_test_list.append(y_level[N_train:])\n",
    "\n",
    "X_train = torch.cat(X_train_list, dim=0)\n",
    "y_train = torch.cat(y_train_list, dim=0)\n",
    "X_test = torch.cat(X_test_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Create noise level labels for training set\n",
    "noise_levels_train_list = []\n",
    "\n",
    "for i, level in enumerate(noise_levels):\n",
    "    noise_level_tensor = torch.full((N_train,), fill_value=i)\n",
    "    noise_levels_train_list.append(noise_level_tensor)\n",
    "\n",
    "noise_levels_train = torch.cat(noise_levels_train_list, dim=0)\n",
    "print(f\"noise_levels_train shape: {noise_levels_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([250000, 3, 32, 32]), y_train: torch.Size([250000])\n",
      "X_test: torch.Size([50000, 3, 32, 32]), y_test: torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "N_train = len(base_train) \n",
    "N_test = len(base_test)\n",
    "\n",
    "samples_per_level = N_train + N_test\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for i in range(len(noise_levels)):\n",
    "    start = i * samples_per_level\n",
    "    end = start + samples_per_level\n",
    "\n",
    "    X_level = augmented_data[start:end]\n",
    "    y_level = augmented_targets[start:end]\n",
    "\n",
    "    X_train_list.append(X_level[:N_train])\n",
    "    y_train_list.append(y_level[:N_train])\n",
    "    X_test_list.append(X_level[N_train:])\n",
    "    y_test_list.append(y_level[N_train:])\n",
    "\n",
    "X_train = torch.cat(X_train_list, dim=0)\n",
    "y_train = torch.cat(y_train_list, dim=0)\n",
    "X_test = torch.cat(X_test_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_levels_train shape: torch.Size([250000])\n"
     ]
    }
   ],
   "source": [
    "noise_levels_train_list = []\n",
    "\n",
    "for i, level in enumerate(noise_levels):\n",
    "    num_train_samples = N_train\n",
    "    noise_level_tensor = torch.full((num_train_samples,), fill_value=i)\n",
    "    \n",
    "    noise_levels_train_list.append(noise_level_tensor)\n",
    "\n",
    "noise_levels_train = torch.cat(noise_levels_train_list, dim=0)\n",
    "\n",
    "print(f\"noise_levels_train shape: {noise_levels_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # random sampling baseline\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_acc = total_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {total_loss/len(train_dataset):.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "test_acc = correct / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.1) Cumulative Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels <= {stage}\")\n",
    "    \n",
    "    stage_mask = noise_levels_train <= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.2) Strict Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAME FAUDRA SANS DOUTE MODIF INITIAL_LAMBDA & LAMBDA_INCREMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')  # Important: per-sample loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "initial_lambda = 1  # initial difficulty threshold\n",
    "lambda_increment = 2  # increase per epoch\n",
    "\n",
    "full_train_dataset = TensorDataset(X_train, y_train)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            losses = criterion(outputs, labels)\n",
    "            all_losses.append(losses.cpu())\n",
    "\n",
    "    all_losses = torch.cat(all_losses)\n",
    "    print(torch.min(all_losses))\n",
    "    print(torch.max(all_losses))\n",
    "\n",
    "    lambda_threshold = initial_lambda + epoch * lambda_increment\n",
    "\n",
    "    selected_indices = (all_losses <= lambda_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"Epoch {epoch+1}: No samples selected for training (lambda={lambda_threshold:.3f}), stopping early.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: lambda={lambda_threshold:.3f}, selected {len(selected_indices)}/{len(X_train)} samples\")\n",
    "\n",
    "    train_subset = TensorDataset(X_train[selected_indices], y_train[selected_indices])\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_subset)\n",
    "    print(f\"  Training Loss: {total_loss/len(train_subset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if len(selected_indices) == len(X_train):\n",
    "        print(f\"  All samples were selected, stopping early.\")\n",
    "        break\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "hard_mask = noise_levels_train >= 3\n",
    "\n",
    "X_hard = X_train[hard_mask]\n",
    "y_hard = y_train[hard_mask]\n",
    "print(f\"Selected {len(X_hard)} hard examples out of {len(X_train)}\")\n",
    "\n",
    "hard_dataset = TensorDataset(X_hard, y_hard)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in hard_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(hard_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(hard_dataset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.1) Cumulative Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels >= {stage}\")\n",
    "    \n",
    "    stage_mask = noise_levels_train >= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.2) Strict Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified Monte Carlo Sampling** is a variance reduction technique where the input space is divided into distinct strata (subregions), and samples are drawn from each stratum. This ensures more uniform coverage of the space compared to standard Monte Carlo sampling, leading to more accurate and stable estimates with fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs_per_stage = 10\n",
    "num_stages = 5\n",
    "samples_per_stage = 10000\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Sampling from stage level = {stage}\")\n",
    "    \n",
    "    stage_mask = (noise_levels_train == stage).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Randomly sample without replacement\n",
    "    if len(stage_mask) < samples_per_stage:\n",
    "        print(f\"  Warning: only {len(stage_mask)} samples available, using all.\")\n",
    "        selected_indices = stage_mask\n",
    "    else:\n",
    "        selected_indices = stage_mask[torch.randperm(len(stage_mask))[:samples_per_stage]]\n",
    "    \n",
    "    X_stage = X_train[selected_indices]\n",
    "    y_stage = y_train[selected_indices]\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
