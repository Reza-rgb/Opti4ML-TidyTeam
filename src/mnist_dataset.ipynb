{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import AddGaussianNoise, AddImpulseNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "full_data = torch.cat([train_dataset.data, test_dataset.data], dim=0)\n",
    "full_targets = torch.cat([train_dataset.targets, test_dataset.targets], dim=0)\n",
    "\n",
    "MNIST_data = full_data.view(len(full_data), -1).numpy().astype('float32') / 255.0\n",
    "\n",
    "MNIST = {\n",
    "    \"data\": MNIST_data,\n",
    "    \"target\": full_targets.numpy()\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmallCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SGD classifer is not enough to get reliable insights on CIFAR-10 dataset so we will use a lightweight CNN. This will allow us to accurately estimate the influence of training order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "X_MNIST = StandardScaler().fit_transform(MNIST[\"data\"])\n",
    "y_MNIST = MNIST[\"target\"]\n",
    "\n",
    "# Split into train and test\n",
    "X_train_MNIST, X_test_MNIST, y_train_MNIST, y_test_MNIST = train_test_split(X_MNIST, y_MNIST, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try as the Vanilla Base Case: Train on the entire dataset using uniform random shuffling for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1/5 - Train Loss: 0.1794\n",
      "Epoch 2/5 - Train Loss: 0.0498\n",
      "Epoch 3/5 - Train Loss: 0.0348\n",
      "Epoch 4/5 - Train Loss: 0.0259\n",
      "Epoch 5/5 - Train Loss: 0.0186\n",
      "Test Accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "# === Training Setup ===\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs = 5\n",
    "\n",
    "# === Training Loop ===\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# === Evaluation ===\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As curriculum learning is based on giving samples in increasing difficulty level to the model, we first need to define a difficulty function. We will base ours on difference between the distance of each point to the centroid of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MNIST_difficulty(X, y, centroids):\n",
    "    dist = np.linalg.norm(X - centroids[y], axis=1)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = np.linspace(0.75, 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy\n",
    "X_train = train_dataset.data.numpy().astype('float32') / 255.0\n",
    "y_train = train_dataset.targets.numpy()\n",
    "\n",
    "# Flatten for difficulty calculation\n",
    "X_train_flat = X_train.reshape(len(X_train), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 6000 samples...\n",
      "Training with 19500 samples...\n",
      "Training with 33000 samples...\n",
      "Training with 46500 samples...\n",
      "Training with 60000 samples...\n"
     ]
    }
   ],
   "source": [
    "# Compute centroids\n",
    "centroids = np.zeros((10, X_train_flat.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_flat[y_train == i].mean(axis=0)\n",
    "\n",
    "# Compute difficulty score\n",
    "difficulties = compute_MNIST_difficulty(X_train_flat, y_train, centroids)\n",
    "\n",
    "# Normalize difficulties to [0, 1]\n",
    "difficulties = (difficulties - difficulties.min()) / (difficulties.max() - difficulties.min())\n",
    "\n",
    "# Sort indices by difficulty\n",
    "sorted_indices = np.argsort(difficulties)\n",
    "\n",
    "# Define your CNN\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert to 3-channel tensors\n",
    "X_train_tensor = torch.tensor(X_train).unsqueeze(1).repeat(1, 3, 1, 1)  # shape: (N, 3, 28, 28)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "# Curriculum schedule (percent of training data to use at each phase)\n",
    "phases = np.linspace(0.1, 1.0, 5)\n",
    "\n",
    "for phase in phases:\n",
    "    n_samples = int(phase * len(sorted_indices))\n",
    "    selected_idx = sorted_indices[:n_samples]\n",
    "\n",
    "    # Create dataloader with increasing difficulty\n",
    "    train_subset = TensorDataset(X_train_tensor[selected_idx], y_train_tensor[selected_idx])\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "    print(f\"Training with {n_samples} samples...\")\n",
    "\n",
    "    # Train for one epoch (or more if you prefer)\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.9879\n"
     ]
    }
   ],
   "source": [
    "# Prepare test set\n",
    "X_test = test_dataset.data.numpy().astype('float32') / 255.0\n",
    "X_test_tensor = torch.tensor(X_test).unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "y_test_tensor = torch.tensor(test_dataset.targets.numpy())\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64)\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Final test accuracy: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Self-Paced Learning, the model is supposed to:\n",
    "\n",
    "• learn from easier samples first (based on current loss)\n",
    "\n",
    "• adaptively expand its training set to include harder samples as it becomes more confident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No samples selected at threshold 0.5\n",
      "No samples selected at threshold 0.8\n",
      "No samples selected at threshold 1.1\n",
      "No samples selected at threshold 1.4000000000000001\n",
      "No samples selected at threshold 1.7000000000000002\n",
      "No samples selected at threshold 2.0\n",
      "Step 7: λ = 2.30, using 24377 samples\n",
      "Step 8: λ = 2.60, using 60000 samples\n",
      "Step 9: λ = 2.90, using 60000 samples\n",
      "Step 10: λ = 3.20, using 37536 samples\n",
      "Step 11: λ = 3.50, using 38004 samples\n",
      "Step 12: λ = 3.80, using 60000 samples\n",
      "Step 13: λ = 4.10, using 60000 samples\n",
      "Step 14: λ = 4.40, using 48752 samples\n",
      "Step 15: λ = 4.70, using 49912 samples\n",
      "Step 16: λ = 5.00, using 60000 samples\n",
      "Step 17: λ = 5.30, using 60000 samples\n",
      "Step 18: λ = 5.60, using 50009 samples\n",
      "Step 19: λ = 5.90, using 50108 samples\n",
      "Step 20: λ = 6.20, using 60000 samples\n",
      "\n",
      "Test accuracy after SPL: 0.9918\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_train_MNIST)\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).view(-1, 1, 28, 28)  # MNIST is 1 channel\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).view(-1, 1, 28, 28)\n",
    "y_test_tensor = torch.tensor(test_dataset.targets.numpy())\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # Per-sample loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Self-Paced Learning parameters\n",
    "# lambda t : init threshold corresponding to the maximum loss (which then determines difficulty)\n",
    "# lambda growth : incrementation at every step\n",
    "# the model is then able to decide how many samples of such difficilty it can handle\n",
    "lambda_threshold = 0.5\n",
    "lambda_step = 0.3\n",
    "max_lambda = 10.0\n",
    "epochs_per_step = 1\n",
    "\n",
    "X_train_all = X_train_tensor.to(device)\n",
    "y_train_all = y_train_tensor.to(device)\n",
    "\n",
    "for step in range(20):  # Enough steps to reach max_lambda\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_train_all)\n",
    "        losses = criterion(outputs, y_train_all)  # shape: (N,)\n",
    "        selected_indices = (losses < lambda_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(selected_indices) == 0:\n",
    "        print(\"No samples selected at threshold\", lambda_threshold)\n",
    "        lambda_threshold += lambda_step\n",
    "        continue\n",
    "\n",
    "    # Training only on selected samples\n",
    "    selected_X = X_train_all[selected_indices]\n",
    "    selected_y = y_train_all[selected_indices]\n",
    "\n",
    "    dataset = TensorDataset(selected_X.cpu(), selected_y.cpu())\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs_per_step):\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f\"Step {step+1}: λ = {lambda_threshold:.2f}, using {len(selected_indices)} samples\")\n",
    "\n",
    "    lambda_threshold += lambda_step\n",
    "    if lambda_threshold > max_lambda:\n",
    "        break\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor.to(device))\n",
    "    test_preds = test_outputs.argmax(dim=1)\n",
    "    accuracy = (test_preds == y_test_tensor.to(device)).float().mean().item()\n",
    "    print(f\"\\nTest accuracy after SPL: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard-Example Mining consists in feeding the model only hard examples. In our case, we will consider that a sample is difficult if its normalized difficulty is greater or equal than 0,75 (in other words the top 25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 337 hard examples out of 60000\n",
      "Epoch 1/5, Loss: 2.2376, Accuracy: 0.1662\n",
      "Epoch 2/5, Loss: 2.0895, Accuracy: 0.3828\n",
      "Epoch 3/5, Loss: 1.8972, Accuracy: 0.4065\n",
      "Epoch 4/5, Loss: 1.6106, Accuracy: 0.4926\n",
      "Epoch 5/5, Loss: 1.3202, Accuracy: 0.6083\n"
     ]
    }
   ],
   "source": [
    "# Normalize difficulty scores to [0, 1]\n",
    "difficulty_scores = compute_MNIST_difficulty(X_train_flat, y_train, centroids)\n",
    "difficulty_scores = (difficulty_scores - difficulty_scores.min()) / (difficulty_scores.max() - difficulty_scores.min())\n",
    "\n",
    "# Select top 25% hardest samples (difficulty >= 0.75)\n",
    "hard_mask = difficulty_scores >= 0.75\n",
    "X_hard = X_train_tensor[hard_mask]\n",
    "y_hard = y_train_tensor[hard_mask]\n",
    "\n",
    "print(f\"Selected {len(X_hard)} hard examples out of {len(X_train_flat)}\")\n",
    "\n",
    "# Create DataLoader for hard examples\n",
    "batch_size = 64\n",
    "hard_dataset = TensorDataset(X_hard, y_hard)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define model (make sure in_channels=1)\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in hard_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(hard_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(hard_dataset):.4f}, Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are implementing **Reverse Curriculum Learning (RCL)** where the model starts learning from easier goals that are close to the target and gradually works backwards to more challenging starting states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Samples: 15000, Loss: 0.6017, Accuracy: 0.8060\n",
      "Epoch 2/5, Samples: 26250, Loss: 0.1334, Accuracy: 0.9590\n",
      "Epoch 3/5, Samples: 37500, Loss: 0.0728, Accuracy: 0.9776\n",
      "Epoch 4/5, Samples: 48750, Loss: 0.0451, Accuracy: 0.9860\n",
      "Epoch 5/5, Samples: 60000, Loss: 0.0306, Accuracy: 0.9903\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute normalized difficulty scores\n",
    "difficulty_scores = compute_MNIST_difficulty(X_train_flat, y_train, centroids)\n",
    "difficulty_scores = (difficulty_scores - difficulty_scores.min()) / (difficulty_scores.max() - difficulty_scores.min())\n",
    "\n",
    "# Step 2: Sort samples by difficulty (descending)\n",
    "sorted_indices = torch.argsort(torch.tensor(difficulty_scores), descending=True)\n",
    "X_sorted = X_train_tensor[sorted_indices]\n",
    "y_sorted = y_train_tensor[sorted_indices]\n",
    "\n",
    "# Step 3: Reverse curriculum training\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "min_fraction = 0.25  # Start with top 25%\n",
    "max_fraction = 1.0   # End with all examples\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Compute current fraction of data to use\n",
    "    frac = min_fraction + (max_fraction - min_fraction) * (epoch / (epochs - 1))\n",
    "    n_samples = int(len(X_sorted) * frac)\n",
    "\n",
    "    # Select subset of data\n",
    "    X_subset = X_sorted[:n_samples]\n",
    "    y_subset = y_sorted[:n_samples]\n",
    "\n",
    "    # Create DataLoader\n",
    "    subset_dataset = TensorDataset(X_subset, y_subset)\n",
    "    subset_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training on current subset\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in subset_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / n_samples\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Samples: {n_samples}, Loss: {total_loss/n_samples:.4f}, Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified Monte Carlo Sampling** is a variance reduction technique where the input space is divided into distinct strata (subregions), and samples are drawn from each stratum. This ensures more uniform coverage of the space compared to standard Monte Carlo sampling, leading to more accurate and stable estimates with fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.9636, Accuracy: 0.6957\n",
      "Epoch 2/5, Loss: 0.2991, Accuracy: 0.9103\n",
      "Epoch 3/5, Loss: 0.1975, Accuracy: 0.9429\n",
      "Epoch 4/5, Loss: 0.1354, Accuracy: 0.9575\n",
      "Epoch 5/5, Loss: 0.1003, Accuracy: 0.9690\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Normalize difficulty scores\n",
    "difficulty_scores = compute_MNIST_difficulty(X_train_flat, y_train, centroids)\n",
    "difficulty_scores = (difficulty_scores - difficulty_scores.min()) / (difficulty_scores.max() - difficulty_scores.min())\n",
    "\n",
    "# Step 2: Define strata (easy = 0-0.33, medium = 0.33-0.66, hard = 0.66-1.0)\n",
    "easy_mask = (difficulty_scores < 0.33)\n",
    "medium_mask = (difficulty_scores >= 0.33) & (difficulty_scores < 0.66)\n",
    "hard_mask = (difficulty_scores >= 0.66)\n",
    "\n",
    "X_easy, y_easy = X_train_tensor[easy_mask], y_train_tensor[easy_mask]\n",
    "X_medium, y_medium = X_train_tensor[medium_mask], y_train_tensor[medium_mask]\n",
    "X_hard, y_hard = X_train_tensor[hard_mask], y_train_tensor[hard_mask]\n",
    "\n",
    "# Step 3: Model, criterion, optimizer\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 4: Training loop with stratified sampling\n",
    "batch_size = 64\n",
    "samples_per_stratum = 2000  # can adjust based on data size\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Stratified random sample from each difficulty bin\n",
    "    indices_easy = torch.randperm(len(X_easy))[:samples_per_stratum]\n",
    "    indices_medium = torch.randperm(len(X_medium))[:samples_per_stratum]\n",
    "    indices_hard = torch.randperm(len(X_hard))[:samples_per_stratum]\n",
    "\n",
    "    X_sample = torch.cat([X_easy[indices_easy], X_medium[indices_medium], X_hard[indices_hard]])\n",
    "    y_sample = torch.cat([y_easy[indices_easy], y_medium[indices_medium], y_hard[indices_hard]])\n",
    "\n",
    "    # Create DataLoader\n",
    "    sampled_dataset = TensorDataset(X_sample, y_sample)\n",
    "    loader = DataLoader(sampled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training on sampled data\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(sampled_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(sampled_dataset):.4f}, Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset with Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a range of increasing difficulty. \n",
    "- 0.0: no noise — easiest samples\n",
    "- 0.4: very noisy — hardest samples\n",
    "- 0.5+ usually makes MNIST unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base MNIST (no transform)\n",
    "base_train = datasets.MNIST(root='./data', train=True, download=True)\n",
    "base_test = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "full_data = torch.cat([base_train.data, base_test.data], dim=0).float() / 255.0\n",
    "full_targets = torch.cat([base_train.targets, base_test.targets], dim=0)\n",
    "\n",
    "# Expand with noise\n",
    "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "augmented_data, augmented_targets, noise_labels = [], [], []\n",
    "\n",
    "for level in noise_levels:\n",
    "    noise = torch.randn_like(full_data) * level\n",
    "    noisy_data = full_data + noise\n",
    "    noisy_data = torch.clamp(noisy_data, 0.0, 1.0)\n",
    "\n",
    "    augmented_data.append(noisy_data)\n",
    "    augmented_targets.append(full_targets)\n",
    "    noise_labels.append(torch.full_like(full_targets, level))\n",
    "\n",
    "# Combine everything\n",
    "augmented_data = torch.cat(augmented_data, dim=0).unsqueeze(1)  # (N*L, 1, 28, 28)\n",
    "augmented_targets = torch.cat(augmented_targets, dim=0)\n",
    "noise_labels = torch.cat(noise_labels, dim=0)\n",
    "\n",
    "# Final dataset\n",
    "augmented_dataset = TensorDataset(augmented_data, augmented_targets, noise_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([300000, 1, 28, 28]), y_train: torch.Size([300000])\n",
      "X_test: torch.Size([50000, 1, 28, 28]), y_test: torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "# Sizes of original MNIST splits\n",
    "N_train = len(base_train)  # 60000\n",
    "N_test = len(base_test)    # 10000\n",
    "\n",
    "# Augmented data shape: [num_levels * (N_train + N_test), 1, 28, 28]\n",
    "samples_per_level = N_train + N_test\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for i in range(len(noise_levels)):\n",
    "    start = i * samples_per_level\n",
    "    end = start + samples_per_level\n",
    "\n",
    "    # Get this noise level's full data and split it\n",
    "    X_level = augmented_data[start:end]\n",
    "    y_level = augmented_targets[start:end]\n",
    "\n",
    "    X_train_list.append(X_level[:N_train])\n",
    "    y_train_list.append(y_level[:N_train])\n",
    "    X_test_list.append(X_level[N_train:])\n",
    "    y_test_list.append(y_level[N_train:])\n",
    "\n",
    "# Final concatenated noisy train/test sets (mixed noise levels)\n",
    "X_train = torch.cat(X_train_list, dim=0)\n",
    "y_train = torch.cat(y_train_list, dim=0)\n",
    "X_test = torch.cat(X_test_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_levels_train shape: torch.Size([300000])\n"
     ]
    }
   ],
   "source": [
    "noise_levels_train_list = []\n",
    "\n",
    "for i, level in enumerate(noise_levels):\n",
    "    # Number of training samples for this noise level\n",
    "    num_train_samples = N_train\n",
    "    \n",
    "    # Create a tensor filled with the noise level index: 0, 1, 2, 3 or 4\n",
    "    noise_level_tensor = torch.full((num_train_samples,), fill_value=i)\n",
    "    \n",
    "    noise_levels_train_list.append(noise_level_tensor)\n",
    "\n",
    "# Concatenate to get noise_levels_train for the entire train set\n",
    "noise_levels_train = torch.cat(noise_levels_train_list, dim=0)\n",
    "\n",
    "print(f\"noise_levels_train shape: {noise_levels_train.shape}\")  # Should be (len(X_train),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Train Loss: 0.1309, Train Acc: 0.9583\n",
      "Test Accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # random sampling baseline\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_acc = total_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {total_loss/len(train_dataset):.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "test_acc = correct / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.1) Cumulative Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using noise levels <= 0\n",
      "  Epoch 1/1 - Loss: 0.1676, Acc: 0.9492\n",
      "\n",
      "Stage 2/5: Using noise levels <= 1\n",
      "  Epoch 1/1 - Loss: 0.0513, Acc: 0.9840\n",
      "\n",
      "Stage 3/5: Using noise levels <= 2\n",
      "  Epoch 1/1 - Loss: 0.0394, Acc: 0.9872\n",
      "\n",
      "Stage 4/5: Using noise levels <= 3\n",
      "  Epoch 1/1 - Loss: 0.0377, Acc: 0.9873\n",
      "\n",
      "Stage 5/5: Using noise levels <= 4\n",
      "  Epoch 1/1 - Loss: 0.0455, Acc: 0.9846\n",
      "\n",
      "Final Test Accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels <= {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train <= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.2) Strict Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using noise level 0\n",
      "  Epoch 1/1 - Loss: 0.1762, Acc: 0.9463\n",
      "\n",
      "Stage 2/5: Using noise level 1\n",
      "  Epoch 1/1 - Loss: 0.0755, Acc: 0.9764\n",
      "\n",
      "Stage 3/5: Using noise level 2\n",
      "  Epoch 1/1 - Loss: 0.0778, Acc: 0.9757\n",
      "\n",
      "Stage 4/5: Using noise level 3\n",
      "  Epoch 1/1 - Loss: 0.1071, Acc: 0.9646\n",
      "\n",
      "Stage 5/5: Using noise level 4\n",
      "  Epoch 1/1 - Loss: 0.1596, Acc: 0.9480\n",
      "\n",
      "Final Test Accuracy: 0.9730\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle.\n",
    "\n",
    "Because our objective is to see the influence of order on the training, we will stop training once the model saw the entire dataset. To be fair with the other techniques where they see the data only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1542)\n",
      "tensor(2.4577)\n",
      "Epoch 1: lambda=2.250, selected 68873/300000 samples\n",
      "  Training Loss: 0.0923, Accuracy: 0.9683\n",
      "tensor(-0.)\n",
      "tensor(41.5899)\n",
      "Epoch 2: lambda=4.250, selected 119147/300000 samples\n",
      "  Training Loss: 0.0321, Accuracy: 0.9885\n",
      "tensor(-0.)\n",
      "tensor(53.7047)\n",
      "Epoch 3: lambda=6.250, selected 120500/300000 samples\n",
      "  Training Loss: 0.0246, Accuracy: 0.9915\n",
      "tensor(-0.)\n",
      "tensor(64.2613)\n",
      "Epoch 4: lambda=8.250, selected 186858/300000 samples\n",
      "  Training Loss: 0.0396, Accuracy: 0.9870\n",
      "tensor(-0.)\n",
      "tensor(63.6075)\n",
      "Epoch 5: lambda=10.250, selected 267750/300000 samples\n",
      "  Training Loss: 0.0400, Accuracy: 0.9865\n",
      "tensor(-0.)\n",
      "tensor(42.1693)\n",
      "Epoch 6: lambda=12.250, selected 299173/300000 samples\n",
      "  Training Loss: 0.0392, Accuracy: 0.9871\n",
      "tensor(-0.)\n",
      "tensor(29.9628)\n",
      "Epoch 7: lambda=14.250, selected 299993/300000 samples\n",
      "  Training Loss: 0.0261, Accuracy: 0.9912\n",
      "tensor(-0.)\n",
      "tensor(26.0948)\n",
      "Epoch 8: lambda=16.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0166, Accuracy: 0.9942\n",
      "tensor(-0.)\n",
      "tensor(29.8680)\n",
      "Epoch 9: lambda=18.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0122, Accuracy: 0.9958\n",
      "tensor(-0.)\n",
      "tensor(32.6593)\n",
      "Epoch 10: lambda=20.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0093, Accuracy: 0.9968\n",
      "\n",
      "Final Test Accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')  # Important: per-sample loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "initial_lambda = 2.25  # initial difficulty threshold\n",
    "lambda_increment = 2  # increase per epoch\n",
    "\n",
    "# Wrap all training data in a dataset/loader for loss evaluation\n",
    "full_train_dataset = TensorDataset(X_train, y_train)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "\n",
    "    # Compute per-sample losses on the full training set\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            losses = criterion(outputs, labels)  # shape: (batch_size,)\n",
    "            all_losses.append(losses.cpu())\n",
    "\n",
    "    all_losses = torch.cat(all_losses)  # shape: (N,)\n",
    "    print(torch.min(all_losses))\n",
    "    print(torch.max(all_losses))\n",
    "\n",
    "    # Determine current lambda threshold\n",
    "    lambda_threshold = initial_lambda + epoch * lambda_increment\n",
    "\n",
    "    # Select indices where loss <= lambda_threshold\n",
    "    selected_indices = (all_losses <= lambda_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"Epoch {epoch+1}: No samples selected for training (lambda={lambda_threshold:.3f}), stopping early.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: lambda={lambda_threshold:.3f}, selected {len(selected_indices)}/{len(X_train)} samples\")\n",
    "\n",
    "    # Create subset dataset and loader for training\n",
    "    train_subset = TensorDataset(X_train[selected_indices], y_train[selected_indices])\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train on selected samples\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()  # mean loss for batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_subset)\n",
    "    print(f\"  Training Loss: {total_loss/len(train_subset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if len(selected_indices) == len(X_train):\n",
    "        print(f\"  All samples were selected, stopping early.\")\n",
    "        break\n",
    "\n",
    "# Evaluate on test set after training\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An idea could be to combine the difficulty scores from the noise levels with the increments in SLP:\n",
    "\n",
    "\n",
    "**Option A: Use Noise Difficulty as a Prior or Weight for Lambda Threshold**\n",
    "\n",
    "- Adjust the SPL threshold (`λ`) for each sample by incorporating its noise difficulty:\n",
    "\n",
    "\\[\n",
    "\\lambda_i = \\lambda_{\\text{base}} + \\alpha \\times \\text{noise\\_level}_i\n",
    "\\]\n",
    "\n",
    "- Samples with higher noise difficulty require a higher loss to be included, effectively entering the curriculum later.\n",
    "\n",
    "**Option B: Use Noise Difficulty for Initial Sample Filtering**\n",
    "\n",
    "- Start SPL training using only samples with noise difficulty below a certain threshold (e.g., noise_level ≤ 0.2).\n",
    "- Gradually expand the training set to include samples with higher noise difficulty as training progresses.\n",
    "\n",
    "**Option C: Weighted Loss or Thresholding by Noise Difficulty Quantiles**\n",
    "\n",
    "- Group samples by their noise difficulty levels.\n",
    "- Compute separate SPL loss thresholds for each noise group.\n",
    "- Allow lower thresholds (easier inclusion) for low-noise groups and higher thresholds for high-noise groups.\n",
    "- This respects both the *a priori* noise difficulty and the *dynamic* training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 120000 hard examples out of 300000\n",
      "Epoch 1/5 - Loss: 0.3404, Accuracy: 0.8916\n",
      "Epoch 2/5 - Loss: 0.1473, Accuracy: 0.9531\n",
      "Epoch 3/5 - Loss: 0.1132, Accuracy: 0.9633\n",
      "Epoch 4/5 - Loss: 0.0923, Accuracy: 0.9703\n",
      "Epoch 5/5 - Loss: 0.0761, Accuracy: 0.9746\n",
      "\n",
      "Final Test Accuracy: 0.9751\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# Define what \"hard\" means: top 1 or 2 noise levels (e.g., 0.3 and 0.4)\n",
    "# If noise_levels_train goes from 0 to 4 (for noise 0.0 to 0.4), we can take levels >= 3\n",
    "hard_mask = noise_levels_train >= 3\n",
    "\n",
    "# Select hard examples\n",
    "X_hard = X_train[hard_mask]\n",
    "y_hard = y_train[hard_mask]\n",
    "print(f\"Selected {len(X_hard)} hard examples out of {len(X_train)}\")\n",
    "\n",
    "# Wrap in dataset and loader\n",
    "hard_dataset = TensorDataset(X_hard, y_hard)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop on hard examples\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in hard_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(hard_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(hard_dataset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.1) Cumulative Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 5/5: Using noise levels >= 4\n",
      "  Epoch 1/1 - Loss: 0.4844, Acc: 0.8424\n",
      "\n",
      "Stage 4/5: Using noise levels >= 3\n",
      "  Epoch 1/1 - Loss: 0.1654, Acc: 0.9469\n",
      "\n",
      "Stage 3/5: Using noise levels >= 2\n",
      "  Epoch 1/1 - Loss: 0.0980, Acc: 0.9680\n",
      "\n",
      "Stage 2/5: Using noise levels >= 1\n",
      "  Epoch 1/1 - Loss: 0.0621, Acc: 0.9794\n",
      "\n",
      "Stage 1/5: Using noise levels >= 0\n",
      "  Epoch 1/1 - Loss: 0.0404, Acc: 0.9865\n",
      "\n",
      "Final Test Accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels >= {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train >= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.2) Strict Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 5/5: Using noise level 4\n",
      "  Epoch 1/1 - Loss: 0.5161, Acc: 0.8323\n",
      "\n",
      "Stage 4/5: Using noise level 3\n",
      "  Epoch 1/1 - Loss: 0.1574, Acc: 0.9499\n",
      "\n",
      "Stage 3/5: Using noise level 2\n",
      "  Epoch 1/1 - Loss: 0.0897, Acc: 0.9715\n",
      "\n",
      "Stage 2/5: Using noise level 1\n",
      "  Epoch 1/1 - Loss: 0.0561, Acc: 0.9823\n",
      "\n",
      "Stage 1/5: Using noise level 0\n",
      "  Epoch 1/1 - Loss: 0.0377, Acc: 0.9885\n",
      "\n",
      "Final Test Accuracy: 0.9396\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Sampling from noise level = 0\n",
      "  Epoch 1/1 - Loss: 0.5726, Acc: 0.8297\n",
      "\n",
      "Stage 2/5: Sampling from noise level = 1\n",
      "  Epoch 1/1 - Loss: 0.1966, Acc: 0.9413\n",
      "\n",
      "Stage 3/5: Sampling from noise level = 2\n",
      "  Epoch 1/1 - Loss: 0.1745, Acc: 0.9460\n",
      "\n",
      "Stage 4/5: Sampling from noise level = 3\n",
      "  Epoch 1/1 - Loss: 0.1896, Acc: 0.9387\n",
      "\n",
      "Stage 5/5: Sampling from noise level = 4\n",
      "  Epoch 1/1 - Loss: 0.2754, Acc: 0.9128\n",
      "\n",
      "Final Test Accuracy: 0.9582\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # same as curriculum\n",
    "num_stages = 5        # noise levels 0–4\n",
    "samples_per_stage = 10000  # number of examples to sample from each difficulty level\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Sampling from noise level = {stage}\")\n",
    "    \n",
    "    # Select indices for current noise level\n",
    "    stage_mask = (noise_levels_train == stage).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Randomly sample without replacement\n",
    "    if len(stage_mask) < samples_per_stage:\n",
    "        print(f\"  Warning: only {len(stage_mask)} samples available, using all.\")\n",
    "        selected_indices = stage_mask\n",
    "    else:\n",
    "        selected_indices = stage_mask[torch.randperm(len(stage_mask))[:samples_per_stage]]\n",
    "    \n",
    "    # Prepare subset\n",
    "    X_stage = X_train[selected_indices]\n",
    "    y_stage = y_train[selected_indices]\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset with Impulse Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a range of increasing difficulty. \n",
    "- 0.0: no noise — easiest samples\n",
    "- 0.4: very noisy — hardest samples\n",
    "- 0.5+ usually makes MNIST unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([300000, 1, 28, 28]), y_train: torch.Size([300000])\n",
      "X_test: torch.Size([50000, 1, 28, 28]), y_test: torch.Size([50000])\n",
      "noise_levels_train shape: torch.Size([300000])\n"
     ]
    }
   ],
   "source": [
    "# Load base MNIST (no transform)\n",
    "base_train = datasets.MNIST(root='./data', train=True, download=True)\n",
    "base_test = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# Normalize and concatenate full dataset\n",
    "full_data = torch.cat([base_train.data, base_test.data], dim=0).float() / 255.0  # [70000, 28, 28]\n",
    "full_targets = torch.cat([base_train.targets, base_test.targets], dim=0)\n",
    "\n",
    "# Define noise levels: fraction of pixels affected\n",
    "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "augmented_data, augmented_targets, noise_labels = [], [], []\n",
    "\n",
    "for level in noise_levels:\n",
    "    noisy_data = full_data.clone()\n",
    "    if level > 0:\n",
    "        N, H, W = noisy_data.shape\n",
    "        total_pixels = H * W\n",
    "\n",
    "        num_noisy = int(level * total_pixels)\n",
    "\n",
    "        for i in range(N):\n",
    "            coords = torch.randperm(total_pixels)[:num_noisy]\n",
    "            salt_or_pepper = torch.randint(0, 2, (num_noisy,), dtype=torch.float32)  # 0 or 1\n",
    "            flat_image = noisy_data[i].view(-1)\n",
    "            flat_image[coords] = salt_or_pepper  # 0 for pepper, 1 for salt\n",
    "\n",
    "    augmented_data.append(noisy_data)\n",
    "    augmented_targets.append(full_targets)\n",
    "    noise_labels.append(torch.full_like(full_targets, fill_value=level))\n",
    "\n",
    "# Stack and reshape\n",
    "augmented_data = torch.cat(augmented_data, dim=0).unsqueeze(1)  # [N * L, 1, 28, 28]\n",
    "augmented_targets = torch.cat(augmented_targets, dim=0)\n",
    "noise_labels = torch.cat(noise_labels, dim=0)\n",
    "\n",
    "# Original sizes\n",
    "N_train = len(base_train)\n",
    "N_test = len(base_test)\n",
    "samples_per_level = N_train + N_test\n",
    "\n",
    "# Split per noise level\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for i in range(len(noise_levels)):\n",
    "    start = i * samples_per_level\n",
    "    end = start + samples_per_level\n",
    "\n",
    "    X_level = augmented_data[start:end]\n",
    "    y_level = augmented_targets[start:end]\n",
    "\n",
    "    X_train_list.append(X_level[:N_train])\n",
    "    y_train_list.append(y_level[:N_train])\n",
    "    X_test_list.append(X_level[N_train:])\n",
    "    y_test_list.append(y_level[N_train:])\n",
    "\n",
    "X_train = torch.cat(X_train_list, dim=0)\n",
    "y_train = torch.cat(y_train_list, dim=0)\n",
    "X_test = torch.cat(X_test_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Create noise level labels for training set\n",
    "noise_levels_train_list = []\n",
    "\n",
    "for i, level in enumerate(noise_levels):\n",
    "    noise_level_tensor = torch.full((N_train,), fill_value=i)\n",
    "    noise_levels_train_list.append(noise_level_tensor)\n",
    "\n",
    "noise_levels_train = torch.cat(noise_levels_train_list, dim=0)\n",
    "print(f\"noise_levels_train shape: {noise_levels_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Train Loss: 0.1343, Train Acc: 0.9570\n",
      "Test Accuracy: 0.9753\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # random sampling baseline\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_acc = total_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {total_loss/len(train_dataset):.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "test_acc = correct / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.1) Cumulative Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using noise levels <= 0\n",
      "  Epoch 1/1 - Loss: 0.1861, Acc: 0.9440\n",
      "\n",
      "Stage 2/5: Using noise levels <= 1\n",
      "  Epoch 1/1 - Loss: 0.0521, Acc: 0.9836\n",
      "\n",
      "Stage 3/5: Using noise levels <= 2\n",
      "  Epoch 1/1 - Loss: 0.0376, Acc: 0.9879\n",
      "\n",
      "Stage 4/5: Using noise levels <= 3\n",
      "  Epoch 1/1 - Loss: 0.0372, Acc: 0.9876\n",
      "\n",
      "Stage 5/5: Using noise levels <= 4\n",
      "  Epoch 1/1 - Loss: 0.0451, Acc: 0.9849\n",
      "\n",
      "Final Test Accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels <= {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train <= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.2) Strict Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using noise level 0\n",
      "  Epoch 1/1 - Loss: 0.1814, Acc: 0.9456\n",
      "\n",
      "Stage 2/5: Using noise level 1\n",
      "  Epoch 1/1 - Loss: 0.0740, Acc: 0.9767\n",
      "\n",
      "Stage 3/5: Using noise level 2\n",
      "  Epoch 1/1 - Loss: 0.0755, Acc: 0.9765\n",
      "\n",
      "Stage 4/5: Using noise level 3\n",
      "  Epoch 1/1 - Loss: 0.1009, Acc: 0.9669\n",
      "\n",
      "Stage 5/5: Using noise level 4\n",
      "  Epoch 1/1 - Loss: 0.1569, Acc: 0.9482\n",
      "\n",
      "Final Test Accuracy: 0.9761\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle.\n",
    "\n",
    "Because our objective is to see the influence of order on the training, we will stop training once the model saw the entire dataset. To be fair with the other techniques where they see the data only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1980)\n",
      "tensor(2.4034)\n",
      "Epoch 1: lambda=2.250, selected 55794/300000 samples\n",
      "  Training Loss: 0.0936, Accuracy: 0.9664\n",
      "tensor(-0.)\n",
      "tensor(36.0352)\n",
      "Epoch 2: lambda=4.250, selected 91538/300000 samples\n",
      "  Training Loss: 0.0187, Accuracy: 0.9934\n",
      "tensor(-0.)\n",
      "tensor(52.5265)\n",
      "Epoch 3: lambda=6.250, selected 91584/300000 samples\n",
      "  Training Loss: 0.0116, Accuracy: 0.9961\n",
      "tensor(-0.)\n",
      "tensor(59.6773)\n",
      "Epoch 4: lambda=8.250, selected 112936/300000 samples\n",
      "  Training Loss: 0.0454, Accuracy: 0.9856\n",
      "tensor(-0.)\n",
      "tensor(71.6176)\n",
      "Epoch 5: lambda=10.250, selected 253725/300000 samples\n",
      "  Training Loss: 0.0603, Accuracy: 0.9798\n",
      "tensor(-0.)\n",
      "tensor(33.8791)\n",
      "Epoch 6: lambda=12.250, selected 299480/300000 samples\n",
      "  Training Loss: 0.0492, Accuracy: 0.9835\n",
      "tensor(-0.)\n",
      "tensor(29.5966)\n",
      "Epoch 7: lambda=14.250, selected 299988/300000 samples\n",
      "  Training Loss: 0.0327, Accuracy: 0.9891\n",
      "tensor(-0.)\n",
      "tensor(32.8389)\n",
      "Epoch 8: lambda=16.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0222, Accuracy: 0.9924\n",
      "tensor(-0.)\n",
      "tensor(33.9541)\n",
      "Epoch 9: lambda=18.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0157, Accuracy: 0.9945\n",
      "tensor(-0.)\n",
      "tensor(44.0749)\n",
      "Epoch 10: lambda=20.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0121, Accuracy: 0.9956\n",
      "\n",
      "Final Test Accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')  # Important: per-sample loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "initial_lambda = 2.25  # initial difficulty threshold\n",
    "lambda_increment = 2  # increase per epoch\n",
    "\n",
    "# Wrap all training data in a dataset/loader for loss evaluation\n",
    "full_train_dataset = TensorDataset(X_train, y_train)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "\n",
    "    # Compute per-sample losses on the full training set\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            losses = criterion(outputs, labels)  # shape: (batch_size,)\n",
    "            all_losses.append(losses.cpu())\n",
    "\n",
    "    all_losses = torch.cat(all_losses)  # shape: (N,)\n",
    "    print(torch.min(all_losses))\n",
    "    print(torch.max(all_losses))\n",
    "\n",
    "    # Determine current lambda threshold\n",
    "    lambda_threshold = initial_lambda + epoch * lambda_increment\n",
    "\n",
    "    # Select indices where loss <= lambda_threshold\n",
    "    selected_indices = (all_losses <= lambda_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"Epoch {epoch+1}: No samples selected for training (lambda={lambda_threshold:.3f}), stopping early.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: lambda={lambda_threshold:.3f}, selected {len(selected_indices)}/{len(X_train)} samples\")\n",
    "\n",
    "    # Create subset dataset and loader for training\n",
    "    train_subset = TensorDataset(X_train[selected_indices], y_train[selected_indices])\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train on selected samples\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()  # mean loss for batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_subset)\n",
    "    print(f\"  Training Loss: {total_loss/len(train_subset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if len(selected_indices) == len(X_train):\n",
    "        print(f\"  All samples were selected, stopping early.\")\n",
    "        break\n",
    "\n",
    "# Evaluate on test set after training\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An idea could be to combine the difficulty scores from the noise levels with the increments in SLP:\n",
    "\n",
    "\n",
    "**Option A: Use Noise Difficulty as a Prior or Weight for Lambda Threshold**\n",
    "\n",
    "- Adjust the SPL threshold (`λ`) for each sample by incorporating its noise difficulty:\n",
    "\n",
    "\\[\n",
    "\\lambda_i = \\lambda_{\\text{base}} + \\alpha \\times \\text{noise\\_level}_i\n",
    "\\]\n",
    "\n",
    "- Samples with higher noise difficulty require a higher loss to be included, effectively entering the curriculum later.\n",
    "\n",
    "**Option B: Use Noise Difficulty for Initial Sample Filtering**\n",
    "\n",
    "- Start SPL training using only samples with noise difficulty below a certain threshold (e.g., noise_level ≤ 0.2).\n",
    "- Gradually expand the training set to include samples with higher noise difficulty as training progresses.\n",
    "\n",
    "**Option C: Weighted Loss or Thresholding by Noise Difficulty Quantiles**\n",
    "\n",
    "- Group samples by their noise difficulty levels.\n",
    "- Compute separate SPL loss thresholds for each noise group.\n",
    "- Allow lower thresholds (easier inclusion) for low-noise groups and higher thresholds for high-noise groups.\n",
    "- This respects both the *a priori* noise difficulty and the *dynamic* training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 120000 hard examples out of 300000\n",
      "Epoch 1/5 - Loss: 0.3107, Accuracy: 0.8999\n",
      "Epoch 2/5 - Loss: 0.1357, Accuracy: 0.9560\n",
      "Epoch 3/5 - Loss: 0.1021, Accuracy: 0.9666\n",
      "Epoch 4/5 - Loss: 0.0814, Accuracy: 0.9735\n",
      "Epoch 5/5 - Loss: 0.0652, Accuracy: 0.9782\n",
      "\n",
      "Final Test Accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# Define what \"hard\" means: top 1 or 2 noise levels (e.g., 0.3 and 0.4)\n",
    "# If noise_levels_train goes from 0 to 4 (for noise 0.0 to 0.4), we can take levels >= 3\n",
    "hard_mask = noise_levels_train >= 3\n",
    "\n",
    "# Select hard examples\n",
    "X_hard = X_train[hard_mask]\n",
    "y_hard = y_train[hard_mask]\n",
    "print(f\"Selected {len(X_hard)} hard examples out of {len(X_train)}\")\n",
    "\n",
    "# Wrap in dataset and loader\n",
    "hard_dataset = TensorDataset(X_hard, y_hard)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop on hard examples\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in hard_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(hard_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(hard_dataset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.1) Cumulative Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 5/5: Using noise levels >= 4\n",
      "  Epoch 1/1 - Loss: 0.5953, Acc: 0.8060\n",
      "\n",
      "Stage 4/5: Using noise levels >= 3\n",
      "  Epoch 1/1 - Loss: 0.2008, Acc: 0.9357\n",
      "\n",
      "Stage 3/5: Using noise levels >= 2\n",
      "  Epoch 1/1 - Loss: 0.1235, Acc: 0.9603\n",
      "\n",
      "Stage 2/5: Using noise levels >= 1\n",
      "  Epoch 1/1 - Loss: 0.0852, Acc: 0.9719\n",
      "\n",
      "Stage 1/5: Using noise levels >= 0\n",
      "  Epoch 1/1 - Loss: 0.0614, Acc: 0.9797\n",
      "\n",
      "Final Test Accuracy: 0.9734\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels >= {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train >= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.2) Strict Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 5/5: Using noise level 4\n",
      "  Epoch 1/1 - Loss: 0.4560, Acc: 0.8542\n",
      "\n",
      "Stage 4/5: Using noise level 3\n",
      "  Epoch 1/1 - Loss: 0.1428, Acc: 0.9552\n",
      "\n",
      "Stage 3/5: Using noise level 2\n",
      "  Epoch 1/1 - Loss: 0.0800, Acc: 0.9749\n",
      "\n",
      "Stage 2/5: Using noise level 1\n",
      "  Epoch 1/1 - Loss: 0.0504, Acc: 0.9842\n",
      "\n",
      "Stage 1/5: Using noise level 0\n",
      "  Epoch 1/1 - Loss: 0.0343, Acc: 0.9892\n",
      "\n",
      "Final Test Accuracy: 0.9454\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Sampling from noise level = 0\n",
      "  Epoch 1/1 - Loss: 0.5492, Acc: 0.8339\n",
      "\n",
      "Stage 2/5: Sampling from noise level = 1\n",
      "  Epoch 1/1 - Loss: 0.1998, Acc: 0.9399\n",
      "\n",
      "Stage 3/5: Sampling from noise level = 2\n",
      "  Epoch 1/1 - Loss: 0.1732, Acc: 0.9461\n",
      "\n",
      "Stage 4/5: Sampling from noise level = 3\n",
      "  Epoch 1/1 - Loss: 0.1909, Acc: 0.9376\n",
      "\n",
      "Stage 5/5: Sampling from noise level = 4\n",
      "  Epoch 1/1 - Loss: 0.2607, Acc: 0.9171\n",
      "\n",
      "Final Test Accuracy: 0.9534\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # same as curriculum\n",
    "num_stages = 5        # noise levels 0–4\n",
    "samples_per_stage = 10000  # number of examples to sample from each difficulty level\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Sampling from noise level = {stage}\")\n",
    "    \n",
    "    # Select indices for current noise level\n",
    "    stage_mask = (noise_levels_train == stage).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Randomly sample without replacement\n",
    "    if len(stage_mask) < samples_per_stage:\n",
    "        print(f\"  Warning: only {len(stage_mask)} samples available, using all.\")\n",
    "        selected_indices = stage_mask\n",
    "    else:\n",
    "        selected_indices = stage_mask[torch.randperm(len(stage_mask))[:samples_per_stage]]\n",
    "    \n",
    "    # Prepare subset\n",
    "    X_stage = X_train[selected_indices]\n",
    "    y_stage = y_train[selected_indices]\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
