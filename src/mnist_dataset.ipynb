{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import AddGaussianNoise, AddImpulseNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Defintion: SmallCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SGD classifer is not enough to get reliable insights on CIFAR-10 dataset so we will use a lightweight CNN. This will allow us to accurately estimate the influence of training order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset (Vanilla Case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([60000, 1, 28, 28]), y_train: torch.Size([60000])\n",
      "X_test: torch.Size([10000, 1, 28, 28]), y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean & std\n",
    "])\n",
    "\n",
    "# Load base MNIST using transform\n",
    "base_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "base_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Convert datasets to tensors\n",
    "X_train = torch.stack([base_train[i][0] for i in range(len(base_train))])  # (60000, 1, 28, 28)\n",
    "y_train = torch.tensor([base_train[i][1] for i in range(len(base_train))]) # (60000,)\n",
    "\n",
    "X_test = torch.stack([base_test[i][0] for i in range(len(base_test))])     # (10000, 1, 28, 28)\n",
    "y_test = torch.tensor([base_test[i][1] for i in range(len(base_test))])    # (10000,)\n",
    "\n",
    "# Optional sanity check\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try as the Vanilla Base Case: Train on the entire dataset using uniform random shuffling for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1/1 - Train Loss: 0.1435\n",
      "Test Accuracy: 0.9876\n"
     ]
    }
   ],
   "source": [
    "# === Training Setup ===\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs = 1\n",
    "\n",
    "# === Training Loop ===\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# === Evaluation ===\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As curriculum learning is based on giving samples in increasing difficulty level to the model, we first need to define a difficulty function. We will base ours on difference between the distance of each point to the centroid of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MNIST_difficulty(X, y, centroids):\n",
    "    dist = np.linalg.norm(X - centroids[y], axis=1)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten for difficulty calculation\n",
    "X_train_flat = X_train.view(len(X_train), -1).numpy()\n",
    "\n",
    "# Compute centroids\n",
    "centroids = np.zeros((10, X_train_flat.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_flat[y_train == i].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using 6000 new examples\n",
      "  Epoch 1/1 - Loss: 0.1688, Acc: 0.9647\n",
      "\n",
      "Stage 2/5: Using 13500 new examples\n",
      "  Epoch 1/1 - Loss: 0.1152, Acc: 0.9744\n",
      "\n",
      "Stage 3/5: Using 13500 new examples\n",
      "  Epoch 1/1 - Loss: 0.0707, Acc: 0.9768\n",
      "\n",
      "Stage 4/5: Using 13500 new examples\n",
      "  Epoch 1/1 - Loss: 0.0921, Acc: 0.9710\n",
      "\n",
      "Stage 5/5: Using 13500 new examples\n",
      "  Epoch 1/1 - Loss: 0.1396, Acc: 0.9559\n",
      "\n",
      "Final Test Accuracy: 0.9440\n"
     ]
    }
   ],
   "source": [
    "# Compute difficulty score\n",
    "difficulties = compute_MNIST_difficulty(X_train_flat, y_train, centroids)\n",
    "\n",
    "# Normalize difficulties to [0, 1]\n",
    "difficulties = (difficulties - difficulties.min()) / (difficulties.max() - difficulties.min())\n",
    "\n",
    "# Sort indices by difficulty\n",
    "sorted_indices = np.argsort(difficulties)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "# Define curriculum stages as % of dataset (in increasing difficulty)\n",
    "phases = np.linspace(0.1, 1.0, num_stages)\n",
    "\n",
    "previous_n = 0  # Start index for slicing\n",
    "\n",
    "for stage, phase in enumerate(phases):\n",
    "    current_n = int(phase * len(sorted_indices))\n",
    "    selected_idx = sorted_indices[previous_n:current_n]\n",
    "    previous_n = current_n  # Update for next stage\n",
    "    \n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using {len(selected_idx)} new examples\")\n",
    "\n",
    "    # Prepare current stage dataset\n",
    "    X_stage = X_train[selected_idx]\n",
    "    y_stage = y_train[selected_idx]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Self-Paced Learning, the model is supposed to:\n",
    "\n",
    "• learn from easier samples first (based on current loss)\n",
    "\n",
    "• adaptively expand its training set to include harder samples as it becomes more confident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: selected 20000 new samples (total seen: 20000/60000)\n",
      "  Train Loss: 0.1339, Acc: 0.9620\n",
      "Epoch 2: selected 20000 new samples (total seen: 40000/60000)\n",
      "  Train Loss: 0.0329, Acc: 0.9896\n",
      "Epoch 3: selected 20000 new samples (total seen: 60000/60000)\n",
      "  Train Loss: 0.1468, Acc: 0.9640\n",
      "Training complete.\n",
      "\n",
      "Final Test Accuracy: 0.6744\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)  # Your CNN model here\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')  # per-sample loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 3\n",
    "samples_per_epoch = len(X_train) // num_epochs\n",
    "\n",
    "seen_mask = torch.zeros(len(X_train), dtype=torch.bool)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        unseen_indices = (~seen_mask).nonzero(as_tuple=True)[0]\n",
    "        unseen_loader = DataLoader(TensorDataset(X_train[unseen_indices], y_train[unseen_indices]),\n",
    "                                   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        all_losses = []\n",
    "        for images, labels in unseen_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            losses = criterion(outputs, labels)  # shape: (batch,)\n",
    "            all_losses.append(losses.cpu())\n",
    "\n",
    "        all_losses = torch.cat(all_losses)\n",
    "    \n",
    "    # Select top-k easiest unseen samples\n",
    "    k = min(samples_per_epoch, len(unseen_indices))\n",
    "    selected_in_unseen = torch.topk(-all_losses, k).indices  # negative for lowest loss\n",
    "    selected_indices = unseen_indices[selected_in_unseen]\n",
    "\n",
    "    seen_mask[selected_indices] = True\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: selected {len(selected_indices)} new samples (total seen: {seen_mask.sum().item()}/{len(X_train)})\")\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train[selected_indices], y_train[selected_indices]),\n",
    "                              batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train model\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(labels)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_loader.dataset)\n",
    "    print(f\"  Train Loss: {total_loss / len(train_loader.dataset):.4f}, Acc: {acc:.4f}\")\n",
    "    \n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Final test accuracy evaluation\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This poor accuracy can be explained because of the cumulative nature of SPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard-Example Mining consists in feeding the model only hard examples. In our case, we will consider that a sample is difficult if its normalized difficulty is greater or equal than 0,75 (in other words the top 25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 337 hard examples out of 60000 total\n",
      "Epoch 1/10 - Loss: 2.0978, Acc: 0.2997\n",
      "Epoch 2/10 - Loss: 1.5392, Acc: 0.5549\n",
      "Epoch 3/10 - Loss: 1.1558, Acc: 0.5994\n",
      "Epoch 4/10 - Loss: 0.8981, Acc: 0.6914\n",
      "Epoch 5/10 - Loss: 0.7529, Acc: 0.7270\n",
      "Epoch 6/10 - Loss: 0.6194, Acc: 0.7626\n",
      "Epoch 7/10 - Loss: 0.4944, Acc: 0.8427\n",
      "Epoch 8/10 - Loss: 0.3577, Acc: 0.8902\n",
      "Epoch 9/10 - Loss: 0.2801, Acc: 0.9169\n",
      "Epoch 10/10 - Loss: 0.1994, Acc: 0.9347\n",
      "\n",
      "Final Test Accuracy: 0.4076\n"
     ]
    }
   ],
   "source": [
    "# Compute difficulty score and normalize\n",
    "difficulties = compute_MNIST_difficulty(X_train_flat, y_train, centroids)\n",
    "difficulties = (difficulties - difficulties.min()) / (difficulties.max() - difficulties.min())\n",
    "\n",
    "# Select hard examples: top 25% (difficulty >= 0.75)\n",
    "hard_mask = difficulties >= 0.75\n",
    "hard_indices = np.where(hard_mask)[0] # we do not shuffle the indices to train on increasingly difficult samples (adapted CL idea)\n",
    "print(f\"Selected {len(hard_indices)} hard examples out of {len(difficulties)} total\")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Prepare hard-example dataset\n",
    "X_hard = X_train[hard_indices]\n",
    "y_hard = y_train[hard_indices]\n",
    "\n",
    "train_dataset = TensorDataset(X_hard, y_hard)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are implementing **Reverse Curriculum Learning (RCL)** where the model starts learning from easier goals that are close to the target and gradually works backwards to more challenging starting states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using 6000 new hard→easy examples\n",
      "  Epoch 1/1 - Loss: 0.7508, Acc: 0.7592\n",
      "\n",
      "Stage 2/5: Using 13500 new hard→easy examples\n",
      "  Epoch 1/1 - Loss: 0.1388, Acc: 0.9576\n",
      "\n",
      "Stage 3/5: Using 13500 new hard→easy examples\n",
      "  Epoch 1/1 - Loss: 0.0603, Acc: 0.9813\n",
      "\n",
      "Stage 4/5: Using 13500 new hard→easy examples\n",
      "  Epoch 1/1 - Loss: 0.0277, Acc: 0.9913\n",
      "\n",
      "Stage 5/5: Using 13500 new hard→easy examples\n",
      "  Epoch 1/1 - Loss: 0.0046, Acc: 0.9989\n",
      "\n",
      "Final Test Accuracy: 0.9327\n"
     ]
    }
   ],
   "source": [
    "# Compute difficulty score\n",
    "difficulties = compute_MNIST_difficulty(X_train_flat, y_train, centroids)\n",
    "\n",
    "# Normalize difficulties to [0, 1]\n",
    "difficulties = (difficulties - difficulties.min()) / (difficulties.max() - difficulties.min())\n",
    "\n",
    "# Sort indices by difficulty — hardest last\n",
    "sorted_indices = np.argsort(difficulties)[::-1].copy()  # Reverse order for hardest first\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1\n",
    "num_stages = 5\n",
    "\n",
    "# Define reverse curriculum stages as % of dataset (from hard to easy)\n",
    "phases = np.linspace(0.1, 1.0, num_stages)  # Percentages\n",
    "\n",
    "previous_n = 0  # Start index for slicing\n",
    "\n",
    "for stage, phase in enumerate(phases):\n",
    "    current_n = int(phase * len(sorted_indices))\n",
    "    selected_idx = sorted_indices[previous_n:current_n]\n",
    "    previous_n = current_n  # Update for next stage\n",
    "    \n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using {len(selected_idx)} new hard→easy examples\")\n",
    "\n",
    "    # Prepare current stage dataset\n",
    "    X_stage = X_train[selected_idx]\n",
    "    y_stage = y_train[selected_idx]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified Monte Carlo Sampling** is a variance reduction technique where the input space is divided into distinct strata (subregions), and samples are drawn from each stratum. This ensures more uniform coverage of the space compared to standard Monte Carlo sampling, leading to more accurate and stable estimates with fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Sampling from all difficulty strata\n",
      "  Epoch 1/1 - Loss: 0.4432, Acc: 0.8607\n",
      "\n",
      "Stage 2/5: Sampling from all difficulty strata\n",
      "  Epoch 1/1 - Loss: 0.1001, Acc: 0.9696\n",
      "\n",
      "Stage 3/5: Sampling from all difficulty strata\n",
      "  Epoch 1/1 - Loss: 0.0828, Acc: 0.9732\n",
      "\n",
      "Stage 4/5: Sampling from all difficulty strata\n",
      "  Epoch 1/1 - Loss: 0.0775, Acc: 0.9781\n",
      "\n",
      "Stage 5/5: Sampling from all difficulty strata\n",
      "  Epoch 1/1 - Loss: 0.0639, Acc: 0.9798\n",
      "\n",
      "Final Test Accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "# Compute difficulty score\n",
    "difficulties = compute_MNIST_difficulty(X_train_flat, y_train, centroids)\n",
    "\n",
    "# Normalize difficulties to [0, 1]\n",
    "difficulties = (difficulties - difficulties.min()) / (difficulties.max() - difficulties.min())\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1\n",
    "num_stages = 5\n",
    "samples_per_stage = int(len(X_train) / num_stages)\n",
    "\n",
    "# Stratify dataset into bins based on difficulty\n",
    "num_bins = num_stages\n",
    "bin_edges = np.linspace(0, 1, num_bins + 1)\n",
    "bins = [[] for _ in range(num_bins)]\n",
    "\n",
    "for idx, score in enumerate(difficulties):\n",
    "    for b in range(num_bins):\n",
    "        if bin_edges[b] <= score < bin_edges[b + 1] or (b == num_bins - 1 and score == 1.0):\n",
    "            bins[b].append(idx)\n",
    "            break\n",
    "\n",
    "# Shuffle each bin\n",
    "for b in bins:\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "# Training loop with stratified sampling\n",
    "seen_indices = set()\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Sampling from all difficulty strata\")\n",
    "\n",
    "    stage_indices = []\n",
    "\n",
    "    for b in bins:\n",
    "        take_n = min(samples_per_stage // num_bins, len(b))\n",
    "        selected = [i for i in b if i not in seen_indices][:take_n]\n",
    "        seen_indices.update(selected)\n",
    "        stage_indices.extend(selected)\n",
    "\n",
    "    np.random.shuffle(stage_indices)\n",
    "\n",
    "    X_stage = X_train[stage_indices]\n",
    "    y_stage = y_train[stage_indices]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset with Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a range of increasing difficulty. \n",
    "- 0.0: no noise — easiest samples\n",
    "- 0.4: very noisy — hardest samples\n",
    "- 0.5+ usually makes MNIST unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base MNIST (no transform)\n",
    "base_train = datasets.MNIST(root='./data', train=True, download=True)\n",
    "base_test = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "full_data = torch.cat([base_train.data, base_test.data], dim=0).float() / 255.0\n",
    "full_targets = torch.cat([base_train.targets, base_test.targets], dim=0)\n",
    "\n",
    "# Expand with noise\n",
    "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "augmented_data, augmented_targets, noise_labels = [], [], []\n",
    "\n",
    "for level in noise_levels:\n",
    "    noise = torch.randn_like(full_data) * level\n",
    "    noisy_data = full_data + noise\n",
    "    noisy_data = torch.clamp(noisy_data, 0.0, 1.0)\n",
    "\n",
    "    augmented_data.append(noisy_data)\n",
    "    augmented_targets.append(full_targets)\n",
    "    noise_labels.append(torch.full_like(full_targets, level))\n",
    "\n",
    "# Combine everything\n",
    "augmented_data = torch.cat(augmented_data, dim=0).unsqueeze(1)  # (N*L, 1, 28, 28)\n",
    "augmented_targets = torch.cat(augmented_targets, dim=0)\n",
    "noise_labels = torch.cat(noise_labels, dim=0)\n",
    "\n",
    "# Final dataset\n",
    "augmented_dataset = TensorDataset(augmented_data, augmented_targets, noise_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([300000, 1, 28, 28]), y_train: torch.Size([300000])\n",
      "X_test: torch.Size([50000, 1, 28, 28]), y_test: torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "# Sizes of original MNIST splits\n",
    "N_train = len(base_train)  # 60000\n",
    "N_test = len(base_test)    # 10000\n",
    "\n",
    "# Augmented data shape: [num_levels * (N_train + N_test), 1, 28, 28]\n",
    "samples_per_level = N_train + N_test\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for i in range(len(noise_levels)):\n",
    "    start = i * samples_per_level\n",
    "    end = start + samples_per_level\n",
    "\n",
    "    # Get this noise level's full data and split it\n",
    "    X_level = augmented_data[start:end]\n",
    "    y_level = augmented_targets[start:end]\n",
    "\n",
    "    X_train_list.append(X_level[:N_train])\n",
    "    y_train_list.append(y_level[:N_train])\n",
    "    X_test_list.append(X_level[N_train:])\n",
    "    y_test_list.append(y_level[N_train:])\n",
    "\n",
    "# Final concatenated noisy train/test sets (mixed noise levels)\n",
    "X_train = torch.cat(X_train_list, dim=0)\n",
    "y_train = torch.cat(y_train_list, dim=0)\n",
    "X_test = torch.cat(X_test_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_levels_train shape: torch.Size([300000])\n"
     ]
    }
   ],
   "source": [
    "noise_levels_train_list = []\n",
    "\n",
    "for i, level in enumerate(noise_levels):\n",
    "    # Number of training samples for this noise level\n",
    "    num_train_samples = N_train\n",
    "    \n",
    "    # Create a tensor filled with the noise level index: 0, 1, 2, 3 or 4\n",
    "    noise_level_tensor = torch.full((num_train_samples,), fill_value=i)\n",
    "    \n",
    "    noise_levels_train_list.append(noise_level_tensor)\n",
    "\n",
    "# Concatenate to get noise_levels_train for the entire train set\n",
    "noise_levels_train = torch.cat(noise_levels_train_list, dim=0)\n",
    "\n",
    "print(f\"noise_levels_train shape: {noise_levels_train.shape}\")  # Should be (len(X_train),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Train Loss: 0.1309, Train Acc: 0.9583\n",
      "Test Accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # random sampling baseline\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_acc = total_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {total_loss/len(train_dataset):.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "test_acc = correct / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.1) Cumulative Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using noise levels <= 0\n",
      "  Epoch 1/1 - Loss: 0.1676, Acc: 0.9492\n",
      "\n",
      "Stage 2/5: Using noise levels <= 1\n",
      "  Epoch 1/1 - Loss: 0.0513, Acc: 0.9840\n",
      "\n",
      "Stage 3/5: Using noise levels <= 2\n",
      "  Epoch 1/1 - Loss: 0.0394, Acc: 0.9872\n",
      "\n",
      "Stage 4/5: Using noise levels <= 3\n",
      "  Epoch 1/1 - Loss: 0.0377, Acc: 0.9873\n",
      "\n",
      "Stage 5/5: Using noise levels <= 4\n",
      "  Epoch 1/1 - Loss: 0.0455, Acc: 0.9846\n",
      "\n",
      "Final Test Accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels <= {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train <= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.2) Strict Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using noise level 0\n",
      "  Epoch 1/1 - Loss: 0.1762, Acc: 0.9463\n",
      "\n",
      "Stage 2/5: Using noise level 1\n",
      "  Epoch 1/1 - Loss: 0.0755, Acc: 0.9764\n",
      "\n",
      "Stage 3/5: Using noise level 2\n",
      "  Epoch 1/1 - Loss: 0.0778, Acc: 0.9757\n",
      "\n",
      "Stage 4/5: Using noise level 3\n",
      "  Epoch 1/1 - Loss: 0.1071, Acc: 0.9646\n",
      "\n",
      "Stage 5/5: Using noise level 4\n",
      "  Epoch 1/1 - Loss: 0.1596, Acc: 0.9480\n",
      "\n",
      "Final Test Accuracy: 0.9730\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle.\n",
    "\n",
    "Because our objective is to see the influence of order on the training, we will stop training once the model saw the entire dataset. To be fair with the other techniques where they see the data only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1542)\n",
      "tensor(2.4577)\n",
      "Epoch 1: lambda=2.250, selected 68873/300000 samples\n",
      "  Training Loss: 0.0923, Accuracy: 0.9683\n",
      "tensor(-0.)\n",
      "tensor(41.5899)\n",
      "Epoch 2: lambda=4.250, selected 119147/300000 samples\n",
      "  Training Loss: 0.0321, Accuracy: 0.9885\n",
      "tensor(-0.)\n",
      "tensor(53.7047)\n",
      "Epoch 3: lambda=6.250, selected 120500/300000 samples\n",
      "  Training Loss: 0.0246, Accuracy: 0.9915\n",
      "tensor(-0.)\n",
      "tensor(64.2613)\n",
      "Epoch 4: lambda=8.250, selected 186858/300000 samples\n",
      "  Training Loss: 0.0396, Accuracy: 0.9870\n",
      "tensor(-0.)\n",
      "tensor(63.6075)\n",
      "Epoch 5: lambda=10.250, selected 267750/300000 samples\n",
      "  Training Loss: 0.0400, Accuracy: 0.9865\n",
      "tensor(-0.)\n",
      "tensor(42.1693)\n",
      "Epoch 6: lambda=12.250, selected 299173/300000 samples\n",
      "  Training Loss: 0.0392, Accuracy: 0.9871\n",
      "tensor(-0.)\n",
      "tensor(29.9628)\n",
      "Epoch 7: lambda=14.250, selected 299993/300000 samples\n",
      "  Training Loss: 0.0261, Accuracy: 0.9912\n",
      "tensor(-0.)\n",
      "tensor(26.0948)\n",
      "Epoch 8: lambda=16.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0166, Accuracy: 0.9942\n",
      "tensor(-0.)\n",
      "tensor(29.8680)\n",
      "Epoch 9: lambda=18.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0122, Accuracy: 0.9958\n",
      "tensor(-0.)\n",
      "tensor(32.6593)\n",
      "Epoch 10: lambda=20.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0093, Accuracy: 0.9968\n",
      "\n",
      "Final Test Accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')  # Important: per-sample loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "initial_lambda = 2.25  # initial difficulty threshold\n",
    "lambda_increment = 2  # increase per epoch\n",
    "\n",
    "# Wrap all training data in a dataset/loader for loss evaluation\n",
    "full_train_dataset = TensorDataset(X_train, y_train)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "\n",
    "    # Compute per-sample losses on the full training set\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            losses = criterion(outputs, labels)  # shape: (batch_size,)\n",
    "            all_losses.append(losses.cpu())\n",
    "\n",
    "    all_losses = torch.cat(all_losses)  # shape: (N,)\n",
    "    print(torch.min(all_losses))\n",
    "    print(torch.max(all_losses))\n",
    "\n",
    "    # Determine current lambda threshold\n",
    "    lambda_threshold = initial_lambda + epoch * lambda_increment\n",
    "\n",
    "    # Select indices where loss <= lambda_threshold\n",
    "    selected_indices = (all_losses <= lambda_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"Epoch {epoch+1}: No samples selected for training (lambda={lambda_threshold:.3f}), stopping early.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: lambda={lambda_threshold:.3f}, selected {len(selected_indices)}/{len(X_train)} samples\")\n",
    "\n",
    "    # Create subset dataset and loader for training\n",
    "    train_subset = TensorDataset(X_train[selected_indices], y_train[selected_indices])\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train on selected samples\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()  # mean loss for batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_subset)\n",
    "    print(f\"  Training Loss: {total_loss/len(train_subset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if len(selected_indices) == len(X_train):\n",
    "        print(f\"  All samples were selected, stopping early.\")\n",
    "        break\n",
    "\n",
    "# Evaluate on test set after training\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An idea could be to combine the difficulty scores from the noise levels with the increments in SLP:\n",
    "\n",
    "\n",
    "**Option A: Use Noise Difficulty as a Prior or Weight for Lambda Threshold**\n",
    "\n",
    "- Adjust the SPL threshold (`λ`) for each sample by incorporating its noise difficulty:\n",
    "\n",
    "\\[\n",
    "\\lambda_i = \\lambda_{\\text{base}} + \\alpha \\times \\text{noise\\_level}_i\n",
    "\\]\n",
    "\n",
    "- Samples with higher noise difficulty require a higher loss to be included, effectively entering the curriculum later.\n",
    "\n",
    "**Option B: Use Noise Difficulty for Initial Sample Filtering**\n",
    "\n",
    "- Start SPL training using only samples with noise difficulty below a certain threshold (e.g., noise_level ≤ 0.2).\n",
    "- Gradually expand the training set to include samples with higher noise difficulty as training progresses.\n",
    "\n",
    "**Option C: Weighted Loss or Thresholding by Noise Difficulty Quantiles**\n",
    "\n",
    "- Group samples by their noise difficulty levels.\n",
    "- Compute separate SPL loss thresholds for each noise group.\n",
    "- Allow lower thresholds (easier inclusion) for low-noise groups and higher thresholds for high-noise groups.\n",
    "- This respects both the *a priori* noise difficulty and the *dynamic* training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 120000 hard examples out of 300000\n",
      "Epoch 1/5 - Loss: 0.3404, Accuracy: 0.8916\n",
      "Epoch 2/5 - Loss: 0.1473, Accuracy: 0.9531\n",
      "Epoch 3/5 - Loss: 0.1132, Accuracy: 0.9633\n",
      "Epoch 4/5 - Loss: 0.0923, Accuracy: 0.9703\n",
      "Epoch 5/5 - Loss: 0.0761, Accuracy: 0.9746\n",
      "\n",
      "Final Test Accuracy: 0.9751\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# Define what \"hard\" means: top 1 or 2 noise levels (e.g., 0.3 and 0.4)\n",
    "# If noise_levels_train goes from 0 to 4 (for noise 0.0 to 0.4), we can take levels >= 3\n",
    "hard_mask = noise_levels_train >= 3\n",
    "\n",
    "# Select hard examples\n",
    "X_hard = X_train[hard_mask]\n",
    "y_hard = y_train[hard_mask]\n",
    "print(f\"Selected {len(X_hard)} hard examples out of {len(X_train)}\")\n",
    "\n",
    "# Wrap in dataset and loader\n",
    "hard_dataset = TensorDataset(X_hard, y_hard)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop on hard examples\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in hard_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(hard_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(hard_dataset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.1) Cumulative Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 5/5: Using noise levels >= 4\n",
      "  Epoch 1/1 - Loss: 0.4844, Acc: 0.8424\n",
      "\n",
      "Stage 4/5: Using noise levels >= 3\n",
      "  Epoch 1/1 - Loss: 0.1654, Acc: 0.9469\n",
      "\n",
      "Stage 3/5: Using noise levels >= 2\n",
      "  Epoch 1/1 - Loss: 0.0980, Acc: 0.9680\n",
      "\n",
      "Stage 2/5: Using noise levels >= 1\n",
      "  Epoch 1/1 - Loss: 0.0621, Acc: 0.9794\n",
      "\n",
      "Stage 1/5: Using noise levels >= 0\n",
      "  Epoch 1/1 - Loss: 0.0404, Acc: 0.9865\n",
      "\n",
      "Final Test Accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels >= {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train >= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.2) Strict Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 5/5: Using noise level 4\n",
      "  Epoch 1/1 - Loss: 0.5161, Acc: 0.8323\n",
      "\n",
      "Stage 4/5: Using noise level 3\n",
      "  Epoch 1/1 - Loss: 0.1574, Acc: 0.9499\n",
      "\n",
      "Stage 3/5: Using noise level 2\n",
      "  Epoch 1/1 - Loss: 0.0897, Acc: 0.9715\n",
      "\n",
      "Stage 2/5: Using noise level 1\n",
      "  Epoch 1/1 - Loss: 0.0561, Acc: 0.9823\n",
      "\n",
      "Stage 1/5: Using noise level 0\n",
      "  Epoch 1/1 - Loss: 0.0377, Acc: 0.9885\n",
      "\n",
      "Final Test Accuracy: 0.9396\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Sampling from noise level = 0\n",
      "  Epoch 1/1 - Loss: 0.5726, Acc: 0.8297\n",
      "\n",
      "Stage 2/5: Sampling from noise level = 1\n",
      "  Epoch 1/1 - Loss: 0.1966, Acc: 0.9413\n",
      "\n",
      "Stage 3/5: Sampling from noise level = 2\n",
      "  Epoch 1/1 - Loss: 0.1745, Acc: 0.9460\n",
      "\n",
      "Stage 4/5: Sampling from noise level = 3\n",
      "  Epoch 1/1 - Loss: 0.1896, Acc: 0.9387\n",
      "\n",
      "Stage 5/5: Sampling from noise level = 4\n",
      "  Epoch 1/1 - Loss: 0.2754, Acc: 0.9128\n",
      "\n",
      "Final Test Accuracy: 0.9582\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # same as curriculum\n",
    "num_stages = 5        # noise levels 0–4\n",
    "samples_per_stage = 10000  # number of examples to sample from each difficulty level\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Sampling from noise level = {stage}\")\n",
    "    \n",
    "    # Select indices for current noise level\n",
    "    stage_mask = (noise_levels_train == stage).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Randomly sample without replacement\n",
    "    if len(stage_mask) < samples_per_stage:\n",
    "        print(f\"  Warning: only {len(stage_mask)} samples available, using all.\")\n",
    "        selected_indices = stage_mask\n",
    "    else:\n",
    "        selected_indices = stage_mask[torch.randperm(len(stage_mask))[:samples_per_stage]]\n",
    "    \n",
    "    # Prepare subset\n",
    "    X_stage = X_train[selected_indices]\n",
    "    y_stage = y_train[selected_indices]\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset with Impulse Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a range of increasing difficulty. \n",
    "- 0.0: no noise — easiest samples\n",
    "- 0.4: very noisy — hardest samples\n",
    "- 0.5+ usually makes MNIST unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([300000, 1, 28, 28]), y_train: torch.Size([300000])\n",
      "X_test: torch.Size([50000, 1, 28, 28]), y_test: torch.Size([50000])\n",
      "noise_levels_train shape: torch.Size([300000])\n"
     ]
    }
   ],
   "source": [
    "# Load base MNIST (no transform)\n",
    "base_train = datasets.MNIST(root='./data', train=True, download=True)\n",
    "base_test = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# Normalize and concatenate full dataset\n",
    "full_data = torch.cat([base_train.data, base_test.data], dim=0).float() / 255.0  # [70000, 28, 28]\n",
    "full_targets = torch.cat([base_train.targets, base_test.targets], dim=0)\n",
    "\n",
    "# Define noise levels: fraction of pixels affected\n",
    "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "augmented_data, augmented_targets, noise_labels = [], [], []\n",
    "\n",
    "for level in noise_levels:\n",
    "    noisy_data = full_data.clone()\n",
    "    if level > 0:\n",
    "        N, H, W = noisy_data.shape\n",
    "        total_pixels = H * W\n",
    "\n",
    "        num_noisy = int(level * total_pixels)\n",
    "\n",
    "        for i in range(N):\n",
    "            coords = torch.randperm(total_pixels)[:num_noisy]\n",
    "            salt_or_pepper = torch.randint(0, 2, (num_noisy,), dtype=torch.float32)  # 0 or 1\n",
    "            flat_image = noisy_data[i].view(-1)\n",
    "            flat_image[coords] = salt_or_pepper  # 0 for pepper, 1 for salt\n",
    "\n",
    "    augmented_data.append(noisy_data)\n",
    "    augmented_targets.append(full_targets)\n",
    "    noise_labels.append(torch.full_like(full_targets, fill_value=level))\n",
    "\n",
    "# Stack and reshape\n",
    "augmented_data = torch.cat(augmented_data, dim=0).unsqueeze(1)  # [N * L, 1, 28, 28]\n",
    "augmented_targets = torch.cat(augmented_targets, dim=0)\n",
    "noise_labels = torch.cat(noise_labels, dim=0)\n",
    "\n",
    "# Original sizes\n",
    "N_train = len(base_train)\n",
    "N_test = len(base_test)\n",
    "samples_per_level = N_train + N_test\n",
    "\n",
    "# Split per noise level\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for i in range(len(noise_levels)):\n",
    "    start = i * samples_per_level\n",
    "    end = start + samples_per_level\n",
    "\n",
    "    X_level = augmented_data[start:end]\n",
    "    y_level = augmented_targets[start:end]\n",
    "\n",
    "    X_train_list.append(X_level[:N_train])\n",
    "    y_train_list.append(y_level[:N_train])\n",
    "    X_test_list.append(X_level[N_train:])\n",
    "    y_test_list.append(y_level[N_train:])\n",
    "\n",
    "X_train = torch.cat(X_train_list, dim=0)\n",
    "y_train = torch.cat(y_train_list, dim=0)\n",
    "X_test = torch.cat(X_test_list, dim=0)\n",
    "y_test = torch.cat(y_test_list, dim=0)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Create noise level labels for training set\n",
    "noise_levels_train_list = []\n",
    "\n",
    "for i, level in enumerate(noise_levels):\n",
    "    noise_level_tensor = torch.full((N_train,), fill_value=i)\n",
    "    noise_levels_train_list.append(noise_level_tensor)\n",
    "\n",
    "noise_levels_train = torch.cat(noise_levels_train_list, dim=0)\n",
    "print(f\"noise_levels_train shape: {noise_levels_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Train Loss: 0.1343, Train Acc: 0.9570\n",
      "Test Accuracy: 0.9753\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # random sampling baseline\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_acc = total_correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {total_loss/len(train_dataset):.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "test_acc = correct / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.1) Cumulative Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using noise levels <= 0\n",
      "  Epoch 1/1 - Loss: 0.1861, Acc: 0.9440\n",
      "\n",
      "Stage 2/5: Using noise levels <= 1\n",
      "  Epoch 1/1 - Loss: 0.0521, Acc: 0.9836\n",
      "\n",
      "Stage 3/5: Using noise levels <= 2\n",
      "  Epoch 1/1 - Loss: 0.0376, Acc: 0.9879\n",
      "\n",
      "Stage 4/5: Using noise levels <= 3\n",
      "  Epoch 1/1 - Loss: 0.0372, Acc: 0.9876\n",
      "\n",
      "Stage 5/5: Using noise levels <= 4\n",
      "  Epoch 1/1 - Loss: 0.0451, Acc: 0.9849\n",
      "\n",
      "Final Test Accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels <= {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train <= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.2) Strict Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Using noise level 0\n",
      "  Epoch 1/1 - Loss: 0.1814, Acc: 0.9456\n",
      "\n",
      "Stage 2/5: Using noise level 1\n",
      "  Epoch 1/1 - Loss: 0.0740, Acc: 0.9767\n",
      "\n",
      "Stage 3/5: Using noise level 2\n",
      "  Epoch 1/1 - Loss: 0.0755, Acc: 0.9765\n",
      "\n",
      "Stage 4/5: Using noise level 3\n",
      "  Epoch 1/1 - Loss: 0.1009, Acc: 0.9669\n",
      "\n",
      "Stage 5/5: Using noise level 4\n",
      "  Epoch 1/1 - Loss: 0.1569, Acc: 0.9482\n",
      "\n",
      "Final Test Accuracy: 0.9761\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle.\n",
    "\n",
    "Because our objective is to see the influence of order on the training, we will stop training once the model saw the entire dataset. To be fair with the other techniques where they see the data only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1980)\n",
      "tensor(2.4034)\n",
      "Epoch 1: lambda=2.250, selected 55794/300000 samples\n",
      "  Training Loss: 0.0936, Accuracy: 0.9664\n",
      "tensor(-0.)\n",
      "tensor(36.0352)\n",
      "Epoch 2: lambda=4.250, selected 91538/300000 samples\n",
      "  Training Loss: 0.0187, Accuracy: 0.9934\n",
      "tensor(-0.)\n",
      "tensor(52.5265)\n",
      "Epoch 3: lambda=6.250, selected 91584/300000 samples\n",
      "  Training Loss: 0.0116, Accuracy: 0.9961\n",
      "tensor(-0.)\n",
      "tensor(59.6773)\n",
      "Epoch 4: lambda=8.250, selected 112936/300000 samples\n",
      "  Training Loss: 0.0454, Accuracy: 0.9856\n",
      "tensor(-0.)\n",
      "tensor(71.6176)\n",
      "Epoch 5: lambda=10.250, selected 253725/300000 samples\n",
      "  Training Loss: 0.0603, Accuracy: 0.9798\n",
      "tensor(-0.)\n",
      "tensor(33.8791)\n",
      "Epoch 6: lambda=12.250, selected 299480/300000 samples\n",
      "  Training Loss: 0.0492, Accuracy: 0.9835\n",
      "tensor(-0.)\n",
      "tensor(29.5966)\n",
      "Epoch 7: lambda=14.250, selected 299988/300000 samples\n",
      "  Training Loss: 0.0327, Accuracy: 0.9891\n",
      "tensor(-0.)\n",
      "tensor(32.8389)\n",
      "Epoch 8: lambda=16.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0222, Accuracy: 0.9924\n",
      "tensor(-0.)\n",
      "tensor(33.9541)\n",
      "Epoch 9: lambda=18.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0157, Accuracy: 0.9945\n",
      "tensor(-0.)\n",
      "tensor(44.0749)\n",
      "Epoch 10: lambda=20.250, selected 299995/300000 samples\n",
      "  Training Loss: 0.0121, Accuracy: 0.9956\n",
      "\n",
      "Final Test Accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')  # Important: per-sample loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "initial_lambda = 2.25  # initial difficulty threshold\n",
    "lambda_increment = 2  # increase per epoch\n",
    "\n",
    "# Wrap all training data in a dataset/loader for loss evaluation\n",
    "full_train_dataset = TensorDataset(X_train, y_train)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "\n",
    "    # Compute per-sample losses on the full training set\n",
    "    with torch.no_grad():\n",
    "        for images, labels in full_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            losses = criterion(outputs, labels)  # shape: (batch_size,)\n",
    "            all_losses.append(losses.cpu())\n",
    "\n",
    "    all_losses = torch.cat(all_losses)  # shape: (N,)\n",
    "    print(torch.min(all_losses))\n",
    "    print(torch.max(all_losses))\n",
    "\n",
    "    # Determine current lambda threshold\n",
    "    lambda_threshold = initial_lambda + epoch * lambda_increment\n",
    "\n",
    "    # Select indices where loss <= lambda_threshold\n",
    "    selected_indices = (all_losses <= lambda_threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"Epoch {epoch+1}: No samples selected for training (lambda={lambda_threshold:.3f}), stopping early.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: lambda={lambda_threshold:.3f}, selected {len(selected_indices)}/{len(X_train)} samples\")\n",
    "\n",
    "    # Create subset dataset and loader for training\n",
    "    train_subset = TensorDataset(X_train[selected_indices], y_train[selected_indices])\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train on selected samples\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels).mean()  # mean loss for batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(train_subset)\n",
    "    print(f\"  Training Loss: {total_loss/len(train_subset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if len(selected_indices) == len(X_train):\n",
    "        print(f\"  All samples were selected, stopping early.\")\n",
    "        break\n",
    "\n",
    "# Evaluate on test set after training\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An idea could be to combine the difficulty scores from the noise levels with the increments in SLP:\n",
    "\n",
    "\n",
    "**Option A: Use Noise Difficulty as a Prior or Weight for Lambda Threshold**\n",
    "\n",
    "- Adjust the SPL threshold (`λ`) for each sample by incorporating its noise difficulty:\n",
    "\n",
    "\\[\n",
    "\\lambda_i = \\lambda_{\\text{base}} + \\alpha \\times \\text{noise\\_level}_i\n",
    "\\]\n",
    "\n",
    "- Samples with higher noise difficulty require a higher loss to be included, effectively entering the curriculum later.\n",
    "\n",
    "**Option B: Use Noise Difficulty for Initial Sample Filtering**\n",
    "\n",
    "- Start SPL training using only samples with noise difficulty below a certain threshold (e.g., noise_level ≤ 0.2).\n",
    "- Gradually expand the training set to include samples with higher noise difficulty as training progresses.\n",
    "\n",
    "**Option C: Weighted Loss or Thresholding by Noise Difficulty Quantiles**\n",
    "\n",
    "- Group samples by their noise difficulty levels.\n",
    "- Compute separate SPL loss thresholds for each noise group.\n",
    "- Allow lower thresholds (easier inclusion) for low-noise groups and higher thresholds for high-noise groups.\n",
    "- This respects both the *a priori* noise difficulty and the *dynamic* training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 120000 hard examples out of 300000\n",
      "Epoch 1/5 - Loss: 0.3107, Accuracy: 0.8999\n",
      "Epoch 2/5 - Loss: 0.1357, Accuracy: 0.9560\n",
      "Epoch 3/5 - Loss: 0.1021, Accuracy: 0.9666\n",
      "Epoch 4/5 - Loss: 0.0814, Accuracy: 0.9735\n",
      "Epoch 5/5 - Loss: 0.0652, Accuracy: 0.9782\n",
      "\n",
      "Final Test Accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# Define what \"hard\" means: top 1 or 2 noise levels (e.g., 0.3 and 0.4)\n",
    "# If noise_levels_train goes from 0 to 4 (for noise 0.0 to 0.4), we can take levels >= 3\n",
    "hard_mask = noise_levels_train >= 3\n",
    "\n",
    "# Select hard examples\n",
    "X_hard = X_train[hard_mask]\n",
    "y_hard = y_train[hard_mask]\n",
    "print(f\"Selected {len(X_hard)} hard examples out of {len(X_train)}\")\n",
    "\n",
    "# Wrap in dataset and loader\n",
    "hard_dataset = TensorDataset(X_hard, y_hard)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop on hard examples\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    for images, labels in hard_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = total_correct / len(hard_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(hard_dataset):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.1) Cumulative Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 5/5: Using noise levels >= 4\n",
      "  Epoch 1/1 - Loss: 0.5953, Acc: 0.8060\n",
      "\n",
      "Stage 4/5: Using noise levels >= 3\n",
      "  Epoch 1/1 - Loss: 0.2008, Acc: 0.9357\n",
      "\n",
      "Stage 3/5: Using noise levels >= 2\n",
      "  Epoch 1/1 - Loss: 0.1235, Acc: 0.9603\n",
      "\n",
      "Stage 2/5: Using noise levels >= 1\n",
      "  Epoch 1/1 - Loss: 0.0852, Acc: 0.9719\n",
      "\n",
      "Stage 1/5: Using noise levels >= 0\n",
      "  Epoch 1/1 - Loss: 0.0614, Acc: 0.9797\n",
      "\n",
      "Final Test Accuracy: 0.9734\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise levels >= {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train >= stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4.2) Strict Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 5/5: Using noise level 4\n",
      "  Epoch 1/1 - Loss: 0.4560, Acc: 0.8542\n",
      "\n",
      "Stage 4/5: Using noise level 3\n",
      "  Epoch 1/1 - Loss: 0.1428, Acc: 0.9552\n",
      "\n",
      "Stage 3/5: Using noise level 2\n",
      "  Epoch 1/1 - Loss: 0.0800, Acc: 0.9749\n",
      "\n",
      "Stage 2/5: Using noise level 1\n",
      "  Epoch 1/1 - Loss: 0.0504, Acc: 0.9842\n",
      "\n",
      "Stage 1/5: Using noise level 0\n",
      "  Epoch 1/1 - Loss: 0.0343, Acc: 0.9892\n",
      "\n",
      "Final Test Accuracy: 0.9454\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # can increase to 2–3 if needed\n",
    "num_stages = 5\n",
    "\n",
    "for stage in reversed(range(num_stages)):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Using noise level {stage}\")\n",
    "    \n",
    "    # Select training data up to current noise level\n",
    "    stage_mask = noise_levels_train == stage\n",
    "    X_stage = X_train[stage_mask]\n",
    "    y_stage = y_train[stage_mask]\n",
    "\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 1/5: Sampling from noise level = 0\n",
      "  Epoch 1/1 - Loss: 0.5492, Acc: 0.8339\n",
      "\n",
      "Stage 2/5: Sampling from noise level = 1\n",
      "  Epoch 1/1 - Loss: 0.1998, Acc: 0.9399\n",
      "\n",
      "Stage 3/5: Sampling from noise level = 2\n",
      "  Epoch 1/1 - Loss: 0.1732, Acc: 0.9461\n",
      "\n",
      "Stage 4/5: Sampling from noise level = 3\n",
      "  Epoch 1/1 - Loss: 0.1909, Acc: 0.9376\n",
      "\n",
      "Stage 5/5: Sampling from noise level = 4\n",
      "  Epoch 1/1 - Loss: 0.2607, Acc: 0.9171\n",
      "\n",
      "Final Test Accuracy: 0.9534\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SmallCNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs_per_stage = 1  # same as curriculum\n",
    "num_stages = 5        # noise levels 0–4\n",
    "samples_per_stage = 10000  # number of examples to sample from each difficulty level\n",
    "\n",
    "for stage in range(num_stages):\n",
    "    print(f\"\\nStage {stage+1}/{num_stages}: Sampling from noise level = {stage}\")\n",
    "    \n",
    "    # Select indices for current noise level\n",
    "    stage_mask = (noise_levels_train == stage).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Randomly sample without replacement\n",
    "    if len(stage_mask) < samples_per_stage:\n",
    "        print(f\"  Warning: only {len(stage_mask)} samples available, using all.\")\n",
    "        selected_indices = stage_mask\n",
    "    else:\n",
    "        selected_indices = stage_mask[torch.randperm(len(stage_mask))[:samples_per_stage]]\n",
    "    \n",
    "    # Prepare subset\n",
    "    X_stage = X_train[selected_indices]\n",
    "    y_stage = y_train[selected_indices]\n",
    "    train_dataset = TensorDataset(X_stage, y_stage)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Train loop for this stage\n",
    "    for epoch in range(epochs_per_stage):\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        acc = total_correct / len(train_dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs_per_stage} - Loss: {total_loss/len(train_dataset):.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Final test accuracy\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = total_correct / len(test_dataset)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
