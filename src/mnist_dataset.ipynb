{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Standard scientific Python imports\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Import datasets, classifiers and performance metrics\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets, metrics, svm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "full_data = torch.cat([train_dataset.data, test_dataset.data], dim=0)\n",
    "full_targets = torch.cat([train_dataset.targets, test_dataset.targets], dim=0)\n",
    "\n",
    "MNIST_data = full_data.view(len(full_data), -1).numpy().astype('float32') / 255.0\n",
    "\n",
    "MNIST = {\n",
    "    \"data\": MNIST_data,\n",
    "    \"target\": full_targets.numpy()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "X_MNIST = StandardScaler().fit_transform(MNIST[\"data\"])\n",
    "y_MNIST = MNIST[\"target\"]\n",
    "\n",
    "# Split into train and test\n",
    "X_train_MNIST, X_test_MNIST, y_train_MNIST, y_test_MNIST = train_test_split(X_MNIST, y_MNIST, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try as the Vanilla Base Case: Train on the entire dataset using uniform random shuffling for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the training data with seed 42\n",
    "X_shuffled, y_shuffled = shuffle(X_train_MNIST, y_train_MNIST, random_state=42)\n",
    "\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_samples = int(percentage * len(X_shuffled))\n",
    "    X_curr = X_shuffled[:n_samples]\n",
    "    y_curr = y_shuffled[:n_samples]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(difficulty_range * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(difficulty_range * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"MNIST Dataset Base Case: Random Sampling (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As curriculum learning is based on giving samples in increasing difficulty level to the model, we first need to define a difficulty function. We will base ours on difference between the distance of each point to the line that goes through all the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MNIST_difficulty(X, y, centroids):\n",
    "    dist = np.linalg.norm(X - centroids[y], axis=1)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = np.linspace(0.75, 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class centroids to later apply the difficulty function\n",
    "centroids = np.zeros((10, X_train_MNIST.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_MNIST[y_train_MNIST == digit].mean(axis=0)\n",
    "\n",
    "# normalize the difficulty to [0, 1]\n",
    "MNIST_difficulty = compute_MNIST_difficulty(X_train_MNIST, y_train_MNIST, centroids)\n",
    "norm_MNIST_difficulty = (MNIST_difficulty - MNIST_difficulty.min()) / (MNIST_difficulty.max() - MNIST_difficulty.min())\n",
    "\n",
    "# sort by difficulty\n",
    "sorted_indices = np.argsort(norm_MNIST_difficulty)\n",
    "X_sorted = X_train_MNIST[sorted_indices]\n",
    "y_sorted = y_train_MNIST[sorted_indices]\n",
    "\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_samples = int(percentage * len(X_sorted))\n",
    "    X_curr = X_sorted[:n_samples]\n",
    "    y_curr = y_sorted[:n_samples]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Curriculum Learning on MNIST Dataset (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Self-Paced Learning, the model is supposed to:\n",
    "\n",
    "• learn from easier samples first (based on current loss)\n",
    "\n",
    "• adaptively expand its training set to include harder samples as it becomes more confident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the best combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_spl(X_train, y_train, X_test, y_test, classes, \n",
    "            lambda_t_init, lambda_growth, init_size, max_iters=10, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Init training subset\n",
    "    init_idx = np.random.choice(len(X_train), size=init_size, replace=False)\n",
    "    X_curr = X_train[init_idx]\n",
    "    y_curr = y_train[init_idx]\n",
    "\n",
    "    # Remaining pool\n",
    "    mask = np.ones(len(X_train), dtype=bool)\n",
    "    mask[init_idx] = False\n",
    "    X_masked = X_train[mask]\n",
    "    y_masked = y_train[mask]\n",
    "\n",
    "    # Initial model\n",
    "    clf = SGDClassifier(loss=\"log_loss\", random_state=random_state)\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    lambda_t = lambda_t_init\n",
    "    test_accuracies = []\n",
    "    init_acc = None\n",
    "\n",
    "    for it in range(1, max_iters + 1):\n",
    "        probas = clf.predict_proba(X_masked)\n",
    "        epsilon = 1e-12\n",
    "        correct_class_probs = probas[np.arange(len(y_masked)), y_masked]\n",
    "        sample_losses = -np.log(np.maximum(correct_class_probs, epsilon))\n",
    "        \n",
    "        filter_mask = sample_losses <= lambda_t\n",
    "        if not np.any(filter_mask):\n",
    "            break\n",
    "\n",
    "        X_filtered = X_masked[filter_mask]\n",
    "        y_filtered = y_masked[filter_mask]\n",
    "\n",
    "        # Update train and pool\n",
    "        X_curr = np.vstack([X_curr, X_filtered])\n",
    "        y_curr = np.concatenate([y_curr, y_filtered])\n",
    "        X_masked = X_masked[~filter_mask]\n",
    "        y_masked = y_masked[~filter_mask]\n",
    "\n",
    "        clf.partial_fit(X_filtered, y_filtered)\n",
    "\n",
    "        test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        if it == 1:\n",
    "            init_acc = test_acc  # Store initial test accuracy\n",
    "\n",
    "        lambda_t += lambda_growth\n",
    "\n",
    "    # Improvement over the first iteration\n",
    "    test_accuracies = np.array(test_accuracies)\n",
    "    acc_improvement = np.max(test_accuracies - init_acc) if init_acc is not None else 0.0\n",
    "\n",
    "    return acc_improvement, test_accuracies\n",
    "\n",
    "\n",
    "\n",
    "# Grid search parameters\n",
    "lambda_t_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "lambda_growth_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "init_sizes = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for lambda_t_init, lambda_growth, init_size in product(lambda_t_values, lambda_growth_values, init_sizes):\n",
    "    score, acc_list = run_spl(X_train_MNIST, y_train_MNIST, X_test_MNIST, y_test_MNIST,\n",
    "                              classes=np.unique(y_train_MNIST),\n",
    "                              lambda_t_init=lambda_t_init,\n",
    "                              lambda_growth=lambda_growth,\n",
    "                              init_size=init_size)\n",
    "    \n",
    "    results.append(((lambda_t_init, lambda_growth, init_size), score))\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_params = (lambda_t_init, lambda_growth, init_size)\n",
    "\n",
    "# Print best combination\n",
    "print(f\"Best params: lambda_t={best_params[0]}, lambda_growth={best_params[1]}, init_size={best_params[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train_MNIST)\n",
    "\n",
    "# definition learning parameters\n",
    "# lambda t : init threshold corresponding to the maximum loss (which then determines difficulty)\n",
    "# lambda growth : incrementation at every step\n",
    "# the model is then able to decide how many samples of such difficilty it can handle\n",
    "max_iters = 10\n",
    "lambda_t = 0.25\n",
    "lambda_growth = 0.3\n",
    "\n",
    "# init training on small random subset\n",
    "init_size = 50\n",
    "init_idx = np.random.choice(len(X_train_MNIST), size=init_size, replace=False)\n",
    "X_curr = X_train_MNIST[init_idx]\n",
    "y_curr = y_train_MNIST[init_idx]\n",
    "\n",
    "# remaining pool\n",
    "mask = np.ones(len(X_train_MNIST), dtype=bool)\n",
    "mask[init_idx] = False\n",
    "X_masked = X_train_MNIST[mask]\n",
    "y_masked = y_train_MNIST[mask]\n",
    "\n",
    "# initial fit\n",
    "clf = SGDClassifier(loss=\"log_loss\", random_state=42)\n",
    "clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for it in range(1, max_iters + 1):\n",
    "    # calculation probs & loss per sample\n",
    "    probas = clf.predict_proba(X_masked)\n",
    "    sample_losses = -np.log(probas[np.arange(len(y_masked)), y_masked])\n",
    "\n",
    "    filter_mask = sample_losses <= lambda_t\n",
    "    if not np.any(filter_mask):\n",
    "        break\n",
    "\n",
    "    X_filtered = X_masked[filter_mask]\n",
    "    y_filtered = y_masked[filter_mask]\n",
    "\n",
    "    X_curr = np.vstack([X_curr, X_filtered])\n",
    "    y_curr = np.concatenate([y_curr, y_filtered])\n",
    "\n",
    "    X_masked = X_masked[~filter_mask]\n",
    "    y_masked = y_masked[~filter_mask]\n",
    "\n",
    "    clf.partial_fit(X_filtered, y_filtered)\n",
    "\n",
    "    train_acc = accuracy_score(y_curr, clf.predict(X_curr))\n",
    "    test_acc = accuracy_score(y_test_MNIST, clf.predict(X_test_MNIST))\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    lambda_t += lambda_growth\n",
    "\n",
    "# tracé des courbes d'apprentissage\n",
    "plt.plot(range(1, len(test_accuracies)+1), test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Itération SPL\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Self-Paced Learning on MNIST Dataset (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard-Example Mining consists in feeding the model only hard examples. In our case, we will consider that a sample is difficult if its normalized difficulty is greater or equal than 0,75 (in other words the top 25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(norm_MNIST_difficulty)\n",
    "X_sorted = X_MNIST[sorted_indices]\n",
    "y_sorted = y_MNIST[sorted_indices]\n",
    "\n",
    "# Curriculum training: increase % of training data\n",
    "percentages = np.linspace(0.8, 1, 4)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# test set\n",
    "X_train_all, X_test_MNIST, y_train_all, y_test_MNIST = train_test_split(X_sorted, y_sorted, test_size=0.2, random_state=42)\n",
    "\n",
    "for percentage in percentages:\n",
    "    n_samples = int(percentage * len(X_train_all))\n",
    "    X_train_MNIST = X_train_all[:n_samples]\n",
    "    y_train_MNIST = y_train_all[:n_samples]\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_MNIST, y_train_MNIST)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train_MNIST, clf.predict(X_train_MNIST))\n",
    "    test_acc = accuracy_score(y_test_MNIST, clf.predict(X_test_MNIST))\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "# Plot\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Hard-Example Mining on MNIST Dataset (Logistic Regression)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are implementing **Reverse Curriculum Learning (RCL)** where the model starts learning from easier goals that are close to the target and gradually works backwards to more challenging starting states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class centroids to later apply the difficulty function\n",
    "centroids = np.zeros((10, X_train_MNIST.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_MNIST[y_train_MNIST == i].mean(axis=0)\n",
    "\n",
    "# normalize the difficulty to [0, 1]\n",
    "MNIST_difficulty = compute_MNIST_difficulty(X_train_MNIST, y_train_MNIST, centroids)\n",
    "norm_MNIST_difficulty = (MNIST_difficulty - MNIST_difficulty.min()) / (MNIST_difficulty.max() - MNIST_difficulty.min())\n",
    "\n",
    "# sort by difficulty\n",
    "sorted_indices = np.argsort(norm_MNIST_difficulty)[::-1] ## Reverse order for RCL\n",
    "X_sorted = X_train_MNIST[sorted_indices]\n",
    "y_sorted = y_train_MNIST[sorted_indices]\n",
    "\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_samples = int(percentage * len(X_sorted))\n",
    "    X_curr = X_sorted[:n_samples]\n",
    "    y_curr = y_sorted[:n_samples]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Reverse Curriculum Learning on MNIST Dataset (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratified Monte Carlo Sampling** is a variance reduction technique where the input space is divided into distinct strata (subregions), and samples are drawn from each stratum. This ensures more uniform coverage of the space compared to standard Monte Carlo sampling, leading to more accurate and stable estimates with fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute difficulty and normalize it\n",
    "centroids = np.zeros((10, X_train_MNIST.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_MNIST[y_train_MNIST == i].mean(axis=0)\n",
    "\n",
    "MNIST_difficulty = compute_MNIST_difficulty(X_train_MNIST, y_train_MNIST, centroids)\n",
    "norm_MNIST_difficulty = (MNIST_difficulty - MNIST_difficulty.min()) / (MNIST_difficulty.max() - MNIST_difficulty.min())\n",
    "\n",
    "# stratify based on difficulty\n",
    "n_strata = 10\n",
    "strata_bounds = np.linspace(0, 1, n_strata + 1)\n",
    "strata_indices = [np.where((norm_MNIST_difficulty >= strata_bounds[i]) & \n",
    "                           (norm_MNIST_difficulty < strata_bounds[i+1]))[0]\n",
    "                  for i in range(n_strata)]\n",
    "\n",
    "# training over growing percentages of the dataset\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_total_samples = int(percentage * len(X_train_MNIST))\n",
    "    n_per_stratum = n_total_samples // n_strata\n",
    "\n",
    "    selected_indices = []\n",
    "    for indices in strata_indices:\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        stratum_sample = np.random.choice(indices, min(n_per_stratum, len(indices)), replace=False)\n",
    "        selected_indices.extend(stratum_sample)\n",
    "\n",
    "    X_curr = X_train_MNIST[selected_indices]\n",
    "    y_curr = y_train_MNIST[selected_indices]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Stratified Monte Carlo Sampling on MNIST Dataset (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset with Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour digits c'était :\n",
    "# digits = datasets.load_digits()\n",
    "# n_samples = len(digits.images)\n",
    "# digits_data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# réadapter de sorte à avoir\n",
    "# MNIST_GN_data & MNIST_GN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "X_MNIST_GN = StandardScaler().fit_transform(MNIST_GN_data)\n",
    "y_MNIST_GN = MNIST_GN.target\n",
    "\n",
    "# Split into train and test\n",
    "X_train_MNIST_GN, X_test_MNIST_GN, y_train_MNIST_GN, y_test_MNIST_GN = train_test_split(X_MNIST_GN, y_MNIST_GN, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training data with seed 42\n",
    "X_shuffled, y_shuffled = shuffle(X_train_MNIST_GN, y_train_MNIST_GN, random_state=42)\n",
    "\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST_GN)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_samples = int(percentage * len(X_shuffled))\n",
    "    X_curr = X_shuffled[:n_samples]\n",
    "    y_curr = y_shuffled[:n_samples]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST_GN)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST_GN, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(difficulty_range * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(difficulty_range * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"MNIST Dataset with Gaussian Noise Base Case: Random Sampling (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MNIST_difficulty(X, y, centroids):\n",
    "    dist = np.linalg.norm(X - centroids[y], axis=1)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = np.linspace(0.75, 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class centroids to later apply the difficulty function\n",
    "centroids = np.zeros((10, X_train_MNIST_GN.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_MNIST_GN[y_train_MNIST_GN == i].mean(axis=0)\n",
    "\n",
    "# normalize the difficulty to [0, 1]\n",
    "MNIST_difficulty = compute_MNIST_difficulty(X_train_MNIST_GN, y_train_MNIST_GN, centroids)\n",
    "norm_MNIST_difficulty = (MNIST_difficulty - MNIST_difficulty.min()) / (MNIST_difficulty.max() - MNIST_difficulty.min())\n",
    "\n",
    "# sort by difficulty\n",
    "sorted_indices = np.argsort(norm_MNIST_difficulty)\n",
    "X_sorted = X_train_MNIST_GN[sorted_indices]\n",
    "y_sorted = y_train_MNIST_GN[sorted_indices]\n",
    "\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST_GN)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_samples = int(percentage * len(X_sorted))\n",
    "    X_curr = X_sorted[:n_samples]\n",
    "    y_curr = y_sorted[:n_samples]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST_GN)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST_GN, test_pred))\n",
    "\n",
    "# Plot results\n",
    "plt.plot(difficulty_range * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(difficulty_range * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Curriculum Learning on MNIST Dataset with Gaussian Noise (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the best combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search parameters\n",
    "lambda_t_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "lambda_growth_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "init_sizes = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for lambda_t_init, lambda_growth, init_size in product(lambda_t_values, lambda_growth_values, init_sizes):\n",
    "    score, acc_list = run_spl(X_train_MNIST_GN, y_train_MNIST_GN, X_test_MNIST_GN, y_test_MNIST_GN,\n",
    "                              classes=np.unique(y_train_MNIST_GN),\n",
    "                              lambda_t_init=lambda_t_init,\n",
    "                              lambda_growth=lambda_growth,\n",
    "                              init_size=init_size)\n",
    "    \n",
    "    results.append(((lambda_t_init, lambda_growth, init_size), score))\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_params = (lambda_t_init, lambda_growth, init_size)\n",
    "\n",
    "# Print best combination\n",
    "print(f\"Best params: lambda_t={best_params[0]}, lambda_growth={best_params[1]}, init_size={best_params[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train_MNIST_GN)\n",
    "\n",
    "# definition learning parameters\n",
    "# lambda t : init threshold corresponding to the maximum loss (which then determines difficulty)\n",
    "# lambda growth : incrementation at every step\n",
    "# the model is then able to decide how many samples of such difficilty it can handle\n",
    "max_iters = 10\n",
    "lambda_t = 0.25\n",
    "lambda_growth = 0.3\n",
    "\n",
    "# init training on small random subset\n",
    "init_size = 50\n",
    "init_idx = np.random.choice(len(X_train_MNIST_GN), size=init_size, replace=False)\n",
    "X_curr = X_train_MNIST_GN[init_idx]\n",
    "y_curr = y_train_MNIST_GN[init_idx]\n",
    "\n",
    "# remaining pool\n",
    "mask = np.ones(len(X_train_MNIST_GN), dtype=bool)\n",
    "mask[init_idx] = False\n",
    "X_masked = X_train_MNIST_GN[mask]\n",
    "y_masked = y_train_MNIST_GN[mask]\n",
    "\n",
    "# initial fit\n",
    "clf = SGDClassifier(loss=\"log_loss\", random_state=42)\n",
    "clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for it in range(1, max_iters + 1):\n",
    "    # calculation probs & loss per sample\n",
    "    probas = clf.predict_proba(X_masked)\n",
    "    sample_losses = -np.log(probas[np.arange(len(y_masked)), y_masked])\n",
    "\n",
    "    filter_mask = sample_losses <= lambda_t\n",
    "    if not np.any(filter_mask):\n",
    "        break\n",
    "\n",
    "    X_filtered = X_masked[filter_mask]\n",
    "    y_filtered = y_masked[filter_mask]\n",
    "\n",
    "    X_curr = np.vstack([X_curr, X_filtered])\n",
    "    y_curr = np.concatenate([y_curr, y_filtered])\n",
    "\n",
    "    X_masked = X_masked[~filter_mask]\n",
    "    y_masked = y_masked[~filter_mask]\n",
    "\n",
    "    clf.partial_fit(X_filtered, y_filtered)\n",
    "\n",
    "    train_acc = accuracy_score(y_curr, clf.predict(X_curr))\n",
    "    test_acc = accuracy_score(y_test_MNIST_GN, clf.predict(X_test_MNIST_GN))\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    lambda_t += lambda_growth\n",
    "\n",
    "# tracé des courbes d'apprentissage\n",
    "plt.plot(range(1, len(test_accuracies)+1), test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Itération SPL\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Self-Paced Learning on MNIST Dataset with Gaussian Noise (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(norm_MNIST_difficulty)\n",
    "X_sorted = X_MNIST_GN[sorted_indices]\n",
    "y_sorted = y_MNIST_GN[sorted_indices]\n",
    "\n",
    "# Curriculum training: increase % of training data\n",
    "percentages = np.linspace(0.8, 1, 4)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# test set\n",
    "X_train_all, X_test_MNIST_GN, y_train_all, y_test_MNIST_GN = train_test_split(X_sorted, y_sorted, test_size=0.2, random_state=42)\n",
    "\n",
    "for percentage in percentages:\n",
    "    n_samples = int(percentage * len(X_train_all))\n",
    "    X_train_MNIST_GN = X_train_all[:n_samples]\n",
    "    y_train_MNIST_GN = y_train_all[:n_samples]\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_MNIST_GN, y_train_MNIST_GN)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train_MNIST_GN, clf.predict(X_train_MNIST_GN))\n",
    "    test_acc = accuracy_score(y_test_MNIST_GN, clf.predict(X_test_MNIST_GN))\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "# Plot\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Hard-Example Mining on MNIST Dataset with Gaussian Noise (Logistic Regression)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class centroids to later apply the difficulty function\n",
    "centroids = np.zeros((10, X_train_MNIST_GN.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_MNIST_GN[y_train_MNIST_GN == i].mean(axis=0)\n",
    "\n",
    "# normalize the difficulty to [0, 1]\n",
    "MNIST_difficulty = compute_MNIST_difficulty(X_train_MNIST_GN, y_train_MNIST_GN, centroids)\n",
    "norm_MNIST_difficulty = (MNIST_difficulty - MNIST_difficulty.min()) / (MNIST_difficulty.max() - MNIST_difficulty.min())\n",
    "\n",
    "# sort by difficulty\n",
    "sorted_indices = np.argsort(norm_MNIST_difficulty)[::-1] ## Reverse order for RCL\n",
    "X_sorted = X_train_MNIST_GN[sorted_indices]\n",
    "y_sorted = y_train_MNIST_GN[sorted_indices]\n",
    "\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST_GN)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_samples = int(percentage * len(X_sorted))\n",
    "    X_curr = X_sorted[:n_samples]\n",
    "    y_curr = y_sorted[:n_samples]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST_GN)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST_GN, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Reverse Curriculum Learning on MNIST Dataset with Gaussian Noise (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute difficulty and normalize it\n",
    "centroids = np.zeros((10, X_train_MNIST_GN.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_MNIST_GN[y_train_MNIST_GN == i].mean(axis=0)\n",
    "\n",
    "MNIST_difficulty = compute_MNIST_difficulty(X_train_MNIST_GN, y_train_MNIST_GN, centroids)\n",
    "norm_MNIST_difficulty = (MNIST_difficulty - MNIST_difficulty.min()) / (MNIST_difficulty.max() - MNIST_difficulty.min())\n",
    "\n",
    "# stratify based on difficulty\n",
    "n_strata = 10\n",
    "strata_bounds = np.linspace(0, 1, n_strata + 1)\n",
    "strata_indices = [np.where((norm_MNIST_difficulty >= strata_bounds[i]) & \n",
    "                           (norm_MNIST_difficulty < strata_bounds[i+1]))[0]\n",
    "                  for i in range(n_strata)]\n",
    "\n",
    "# training over growing percentages of the dataset\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST_GN)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_total_samples = int(percentage * len(X_train_MNIST_GN))\n",
    "    n_per_stratum = n_total_samples // n_strata\n",
    "\n",
    "    selected_indices = []\n",
    "    for indices in strata_indices:\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        stratum_sample = np.random.choice(indices, min(n_per_stratum, len(indices)), replace=False)\n",
    "        selected_indices.extend(stratum_sample)\n",
    "\n",
    "    X_curr = X_train_MNIST_GN[selected_indices]\n",
    "    y_curr = y_train_MNIST_GN[selected_indices]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST_GN)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST_GN, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Stratified Monte Carlo Sampling on MNIST Dataset with Gaussian Noise (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset with Impulse Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour digits c'était :\n",
    "# digits = datasets.load_digits()\n",
    "# n_samples = len(digits.images)\n",
    "# digits_data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# réadapter de sorte à avoir\n",
    "# MNIST_IN_data & MNIST_IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Base Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "X_MNIST_IN = StandardScaler().fit_transform(MNIST_IN_data)\n",
    "y_MNIST_IN = MNIST_IN.target\n",
    "\n",
    "# Split into train and test\n",
    "X_train_MNIST_IN, X_test_MNIST_IN, y_train_MNIST_IN, y_test_MNIST_IN = train_test_split(X_MNIST_IN, y_MNIST_IN, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training data with seed 42\n",
    "X_shuffled, y_shuffled = shuffle(X_train_MNIST_IN, y_train_MNIST_IN, random_state=42)\n",
    "\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST_IN)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_samples = int(percentage * len(X_shuffled))\n",
    "    X_curr = X_shuffled[:n_samples]\n",
    "    y_curr = y_shuffled[:n_samples]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST_IN)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST_IN, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(difficulty_range * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(difficulty_range * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"MNIST Dataset with Impulse Noise Base Case: Random Sampling (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MNIST_difficulty(X, y, centroids):\n",
    "    dist = np.linalg.norm(X - centroids[y], axis=1)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = np.linspace(0.75, 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class centroids to later apply the difficulty function\n",
    "centroids = np.zeros((10, X_train_MNIST_IN.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_MNIST_IN[y_train_MNIST_IN == i].mean(axis=0)\n",
    "\n",
    "# normalize the difficulty to [0, 1]\n",
    "MNIST_difficulty = compute_MNIST_difficulty(X_train_MNIST_IN, y_train_MNIST_IN, centroids)\n",
    "norm_MNIST_difficulty = (MNIST_difficulty - MNIST_difficulty.min()) / (MNIST_difficulty.max() - MNIST_difficulty.min())\n",
    "\n",
    "# sort by difficulty\n",
    "sorted_indices = np.argsort(norm_MNIST_difficulty)\n",
    "X_sorted = X_train_MNIST_IN[sorted_indices]\n",
    "y_sorted = y_train_MNIST_IN[sorted_indices]\n",
    "\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST_IN)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_samples = int(percentage * len(X_sorted))\n",
    "    X_curr = X_sorted[:n_samples]\n",
    "    y_curr = y_sorted[:n_samples]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST_IN)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST_IN, test_pred))\n",
    "\n",
    "# Plot results\n",
    "plt.plot(difficulty_range * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(difficulty_range * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Curriculum Learning on MNIST Dataset with Impulse Noise (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Self-Paced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same X_train, X_test, y_train and y_test computed in the curriculum learning phase.\n",
    "To implement the SPL we will introduce a difficulty threshold to let the model choose how many samples of this difficulty it is ready to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the best combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search parameters\n",
    "lambda_t_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "lambda_growth_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "init_sizes = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for lambda_t_init, lambda_growth, init_size in product(lambda_t_values, lambda_growth_values, init_sizes):\n",
    "    score, acc_list = run_spl(X_train_MNIST_IN, y_train_MNIST_IN, X_test_MNIST_IN, y_test_MNIST_IN,\n",
    "                              classes=np.unique(y_train_MNIST_IN),\n",
    "                              lambda_t_init=lambda_t_init,\n",
    "                              lambda_growth=lambda_growth,\n",
    "                              init_size=init_size)\n",
    "    \n",
    "    results.append(((lambda_t_init, lambda_growth, init_size), score))\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_params = (lambda_t_init, lambda_growth, init_size)\n",
    "\n",
    "# Print best combination\n",
    "print(f\"Best params: lambda_t={best_params[0]}, lambda_growth={best_params[1]}, init_size={best_params[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train_MNIST_IN)\n",
    "\n",
    "# definition learning parameters\n",
    "# lambda t : init threshold corresponding to the maximum loss (which then determines difficulty)\n",
    "# lambda growth : incrementation at every step\n",
    "# the model is then able to decide how many samples of such difficilty it can handle\n",
    "max_iters = 10\n",
    "lambda_t = 0.25\n",
    "lambda_growth = 0.3\n",
    "\n",
    "# init training on small random subset\n",
    "init_size = 50\n",
    "init_idx = np.random.choice(len(X_train_MNIST_IN), size=init_size, replace=False)\n",
    "X_curr = X_train_MNIST_IN[init_idx]\n",
    "y_curr = y_train_MNIST_IN[init_idx]\n",
    "\n",
    "# remaining pool\n",
    "mask = np.ones(len(X_train_MNIST_IN), dtype=bool)\n",
    "mask[init_idx] = False\n",
    "X_masked = X_train_MNIST_IN[mask]\n",
    "y_masked = y_train_MNIST_IN[mask]\n",
    "\n",
    "# initial fit\n",
    "clf = SGDClassifier(loss=\"log_loss\", random_state=42)\n",
    "clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for it in range(1, max_iters + 1):\n",
    "    # calculation probs & loss per sample\n",
    "    probas = clf.predict_proba(X_masked)\n",
    "    sample_losses = -np.log(probas[np.arange(len(y_masked)), y_masked])\n",
    "\n",
    "    filter_mask = sample_losses <= lambda_t\n",
    "    if not np.any(filter_mask):\n",
    "        break\n",
    "\n",
    "    X_filtered = X_masked[filter_mask]\n",
    "    y_filtered = y_masked[filter_mask]\n",
    "\n",
    "    X_curr = np.vstack([X_curr, X_filtered])\n",
    "    y_curr = np.concatenate([y_curr, y_filtered])\n",
    "\n",
    "    X_masked = X_masked[~filter_mask]\n",
    "    y_masked = y_masked[~filter_mask]\n",
    "\n",
    "    clf.partial_fit(X_filtered, y_filtered)\n",
    "\n",
    "    train_acc = accuracy_score(y_curr, clf.predict(X_curr))\n",
    "    test_acc = accuracy_score(y_test_MNIST_IN, clf.predict(X_test_MNIST_IN))\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    lambda_t += lambda_growth\n",
    "\n",
    "# tracé des courbes d'apprentissage\n",
    "plt.plot(range(1, len(test_accuracies)+1), test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Itération SPL\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Self-Paced Learning on MNIST Dataset with Impulsive Noise (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Hard-Example Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(norm_MNIST_difficulty)\n",
    "X_sorted = X_MNIST_IN[sorted_indices]\n",
    "y_sorted = y_MNIST_IN[sorted_indices]\n",
    "\n",
    "# Curriculum training: increase % of training data\n",
    "percentages = np.linspace(0.8, 1, 4)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# test set\n",
    "X_train_all, X_test_MNIST_IN, y_train_all, y_test_MNIST_IN = train_test_split(X_sorted, y_sorted, test_size=0.2, random_state=42)\n",
    "\n",
    "for percentage in percentages:\n",
    "    n_samples = int(percentage * len(X_train_all))\n",
    "    X_train_MNIST_IN = X_train_all[:n_samples]\n",
    "    y_train_MNIST_IN = y_train_all[:n_samples]\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_MNIST_IN, y_train_MNIST_IN)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train_MNIST_IN, clf.predict(X_train_MNIST_IN))\n",
    "    test_acc = accuracy_score(y_test_MNIST_IN, clf.predict(X_test_MNIST_IN))\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "# Plot\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Hard-Example Mining on MNIST Dataset with Impulsive Noise (Logistic Regression)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Reverse Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class centroids to later apply the difficulty function\n",
    "centroids = np.zeros((10, X_train_MNIST_IN.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_MNIST_IN[y_train_MNIST_IN == i].mean(axis=0)\n",
    "\n",
    "# normalize the difficulty to [0, 1]\n",
    "MNIST_difficulty = compute_MNIST_difficulty(X_train_MNIST_IN, y_train_MNIST_IN, centroids)\n",
    "norm_MNIST_difficulty = (MNIST_difficulty - MNIST_difficulty.min()) / (MNIST_difficulty.max() - MNIST_difficulty.min())\n",
    "\n",
    "# sort by difficulty\n",
    "sorted_indices = np.argsort(norm_MNIST_difficulty)[::-1] ## Reverse order for RCL\n",
    "X_sorted = X_train_MNIST_IN[sorted_indices]\n",
    "y_sorted = y_train_MNIST_IN[sorted_indices]\n",
    "\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST_IN)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_samples = int(percentage * len(X_sorted))\n",
    "    X_curr = X_sorted[:n_samples]\n",
    "    y_curr = y_sorted[:n_samples]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST_IN)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST_IN, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Reverse Curriculum Learning on MNIST Dataset with Impulsive Noise (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Stratified Monte-Carlo Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute difficulty and normalize it\n",
    "centroids = np.zeros((10, X_train_MNIST_IN.shape[1]))\n",
    "for i in range(10):\n",
    "    centroids[i] = X_train_MNIST_IN[y_train_MNIST_IN == i].mean(axis=0)\n",
    "\n",
    "MNIST_difficulty = compute_MNIST_difficulty(X_train_MNIST_IN, y_train_MNIST_IN, centroids)\n",
    "norm_MNIST_difficulty = (MNIST_difficulty - MNIST_difficulty.min()) / (MNIST_difficulty.max() - MNIST_difficulty.min())\n",
    "\n",
    "# stratify based on difficulty\n",
    "n_strata = 10\n",
    "strata_bounds = np.linspace(0, 1, n_strata + 1)\n",
    "strata_indices = [np.where((norm_MNIST_difficulty >= strata_bounds[i]) & \n",
    "                           (norm_MNIST_difficulty < strata_bounds[i+1]))[0]\n",
    "                  for i in range(n_strata)]\n",
    "\n",
    "# training over growing percentages of the dataset\n",
    "difficulty_range = np.linspace(0.1, 1.0, 10)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "classes = np.unique(y_train_MNIST_IN)\n",
    "\n",
    "for percentage in difficulty_range:\n",
    "    n_total_samples = int(percentage * len(X_train_MNIST_IN))\n",
    "    n_per_stratum = n_total_samples // n_strata\n",
    "\n",
    "    selected_indices = []\n",
    "    for indices in strata_indices:\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        stratum_sample = np.random.choice(indices, min(n_per_stratum, len(indices)), replace=False)\n",
    "        selected_indices.extend(stratum_sample)\n",
    "\n",
    "    X_curr = X_train_MNIST_IN[selected_indices]\n",
    "    y_curr = y_train_MNIST_IN[selected_indices]\n",
    "\n",
    "    clf.partial_fit(X_curr, y_curr, classes=classes)\n",
    "\n",
    "    train_pred = clf.predict(X_curr)\n",
    "    test_pred = clf.predict(X_test_MNIST_IN)\n",
    "\n",
    "    train_accuracies.append(accuracy_score(y_curr, train_pred))\n",
    "    test_accuracies.append(accuracy_score(y_test_MNIST_IN, test_pred))\n",
    "\n",
    "# plot results\n",
    "plt.plot(percentages * 100, test_accuracies, label=\"Test accuracy\")\n",
    "plt.plot(percentages * 100, train_accuracies, label=\"Train accuracy\")\n",
    "plt.xlabel(\"Percentage of training data (by difficulty)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Stratified Monte Carlo Sampling on MNIST Dataset with Impulsive Noise (SGDClassifier)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
